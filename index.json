[{"categories":null,"contents":"üó∫Ô∏è Summary This article will quickly show you the process of obtaining airspace data using the OpenSky REST API and how I was able to visualise on a map. I will focus on North American airspace as it seems this region captures the highest volume of aircraft.\nThe Flight Tracking GitHub Project link is here should you wish to examine my scripts and get some context for the commentary in this article.\nThe totality of the airspace depends on the maintenance of the OpenSky network. The majority of the data is recorded via ADS-B receivers, which are distributed over the land mass.\nRegarding aircraft surveillance, I presume we lose visibility of aircraft (in the data) outside the range of ground stations such as the sea or mountainous terrain. Otherwise it would\u0026rsquo;ve been interesting to monitor the full extent of flight paths across the globe.\nAircraft require a transponder to retrieve the GPS data i.e. communicating between satellites and ground stations. Any aircraft flying with no transponder are not captured in this view.\nI am sure there are other flight data providers selling more accurate views and more reliable aircraft coverage, but this is purely an illustrative exercise for curiosities sake.\nüí¨ Data Processing Commentary I\u0026rsquo;ll go through details of my data collection, storage and processing.\nThis was a fairly quick and dirty version, however, I still ensured a reasonable folder structure to manipulate the data in my workflow.\nLong-term we can look at dumping this info into a SQLite or Postgres database.\nüñºÔ∏è Overall View The workflow looks something like this:\ngraph LR; classDef blue fill:#2374f7,stroke:#8ec1f5,stroke-width:7px, color:#fff,stroke-dasharray: 4 1 classDef yellow fill:#e6d00b,stroke:#f5eb5b,stroke-width:7px, color:#000,stroke-dasharray: 4 1 classDef green fill:#10ad0a,stroke:#7feb4d,stroke-width:7px, color:#000,stroke-dasharray: 4 1 classDef red fill:#db3b1f,stroke:#eb654d,stroke-width:7px, color:#fff,stroke-dasharray: 4 1 1([Request aircraft positions]):::blue --o 2([Store raw csv]):::yellow--o3([Process images]):::green--o4([Create Video]):::red linkStyle 0 stroke:#8ec1f5,stroke-width:11px linkStyle 1 stroke:#f5eb5b,stroke-width:11px linkStyle 2 stroke:#7feb4d,stroke-width:11px API Request \u0026nbsp; Every 60s for worldwide airspace snapshots. Data Storage \u0026nbsp; Dumping series of snapshots into timestamped csv\u0026rsquo;s. Image Processing \u0026nbsp; Read/prepare/filter data and draw visualisation onto map. Animating \u0026nbsp; Splice image collection with ffmpeg shell script ‚è±Ô∏è Procedural Timings To put some perspective in time required to process this data:\nIt takes about 16 hours to make 850 requests (@60s intervals). It takes about 5-10 mins to create all 850 image visualisations with quiver plot. It takes about 20 hours to create all 850 image visualisations with the KDE plot. I\u0026rsquo;m pretty sure the script slowed down over time. I was only removing the quiver artist but not the KDE artist (by omission). Probably too many chart element variables were being stored in the memory over time. Another 4-7 mins to crops all the white space from the images. Splicing images takes about 3-5 ish mins into a neat .mp4. And lastly the video edit takes around 30 mins to overlay music. All in all it roughly takes a full day with the PC running essentially non-stop. And I was manually driving the scripts since I didn\u0026rsquo;t build any monitoring tools in this workflow.\nüöú Data Collection The goal here is simple: recursively request and store the aircraft positions in a list of timestamped csv\u0026rsquo;s. This is how we will build our data repository.\nWe have to balance the time delta between each snapshot (API call); not too large to lose movement details and not too small to overload the API with requests. The latter is probably more important to consider, because we don\u0026rsquo;t want to bombard the API and risk taking it completely out of commission with high frequency requests.\nIt\u0026rsquo;s worth noting that that anonymous OpenSky API users are limited to 80 requests per day. This is no good for us since we are trying to make an extended time lapse over a period of time.\nWe can circumvent this issue through IP address rotation. Each API call will be distributed over a pool of IP addresses and thus prevent our request timing out (since they can\u0026rsquo;t pin down our real identity).\nThis typically requires the use of a paid service to provide a set of high quality proxies. In my scenario, I chose to use OxyLabs which have a dedicated web scrapping proxy tool. You can route your Python request via the OxyLabs web-scrapping tool and it will automagically handle the proxies for you. I was able to run my script overnight with zero failures and retrieved over 850 requests.\nEach request produces a 1.1MB csv file. Over the course of 16 hours, we were able to hoover about 1GB of data all together. Unfortunately, my script crashed in the late stages of the execution probably because I was running it out of a jupyter notebook and recursively printing output messages. Otherwise we could\u0026rsquo;ve captured more. I\u0026rsquo;ll convert it into a Python script eventually, it just so happens its easier to do data exploration in Jupyter.\nYou may notice also that the git history doesn\u0026rsquo;t have an extensive list of csv data which because there is no reason to have a volume of csv\u0026rsquo;s saved on the git history. Therefore it\u0026rsquo;s simply added to the .gitignore and we are storing the files locally for the most part.\nüìä Data Visualisation I think the most interesting component to visualising this type of data is monitoring the density of air traffic at various times of day. Especially in North America where the time zone from east coast to west coast varies by a 6-hour difference. As the country approaches the early hours of the morning, the entire east coast essentially \u0026ldquo;go to sleep\u0026rdquo; with respect to the density of the traffic. Then it flips vice-versa as time progresses.\nAdditionally, we can see commercial flight routes and airport hubs where these flight networks are connecting each other. There is a multitude of ways to visualise this information, all the way from a contourf plot to a quiver plot to some kind of color mesh plot. Alternative 2D visualisations will reveal different information about air traffic behaviour, so it\u0026rsquo;s good to compare and contrast.\nAvailable GPS Data sourced from OpenSky API:\nLongitude Latitude Speed Bearing Various other data points including callsigns. For a quiver plot, we need to split the speed into its horizontal and vertical components. We can do some basic trigonometry to figure that out.\nIn the quiver example below, we see the direction and speed (relative to it\u0026rsquo;s arrow size) of each individual aircraft. When this data is animated, we can see the arrows get smaller for aircraft on the approach, and grow as they are taking off. It has a very interesting appearance. We also can map this out is by creating a scatter distribution and embed the Gaussian KDE (Kernel Density Estimation) mapping onto the individual points themselves to get a feel for the distribution as there is a lot of overlap at this scale:\nAnd we can map the KDE over the entire map itself, however, we must create a \u0026ldquo;meshgrid\u0026rdquo; over the plot. The aircraft exist between the grids native resolution, therefore we would need to re-shape the data into a 2-dimensional array containing the aircraft density distribution.\nWe can combine the 2D KDE mapping with the quiver plot to reveal a bit more information on aircraft density distribution, like so:\nThe plotting process is iterated over the folder of timestamped csv\u0026rsquo;s to generate the incremental aircraft movements over time.\nMap Data Struggles Something that left me completely stumped for a while is that I was not able to directly use the Stadia Maps API within in Cartopy 0.22.0 which was confusing and frustrating.\nI chose to use the Stadia Maps API via the Cartopy as it had already build the classes to plug into the API and it had a high contrast map tile that thought was suitable. But it doesn\u0026rsquo;t work on Cartopy 0.22.0\u0026hellip; It really was a head scratcher for me. After reading the site-packages I realised that the API class I was using was completely defunct within a matter of months.\nAlthough, I discovered a revised class object sitting (which works) in Cartopy 0.23.0 which was unavailable as a wheel/conda package. So, this was the first time I had to go build the package from source in the git repo just for this specific use case. But when they eventually package the final version of 0.23.0 in conda, this issue will no longer be the case\u0026hellip;\nAlthough I came across some issues since each API call for the mapping data costs credits to use. And since the map is completely static, it doesn\u0026rsquo;t make sense to make multiple API calls for each set of data from a cost perspective.\nEven from a time perspective, it takes a very long time to render the map, about 13s for each snapshot which is way too long. Instead, the process I have created is to only clear the relevant \u0026ldquo;artists\u0026rdquo; on the matplotlib figure whilst retaining the map image on the plot. So we can save a lot of time.\nPlotting the scatterpoints takes a trivial amount of time in comparison, about 1-2s or so per iteration.\nSlow Color Mesh Plotting Plotting the 2D Gaussian KDE plot was much more intensive as we are drawing a high resolution color mapping across the entire grid. Each iteration was about 20-25s to render but it tended to slow down to over 1m per iteration. The filled color grid mapping took about 20 hours to create all 850 images. Probably because I was not removing the artist itself per iteration, but this is just a working assumption. I\u0026rsquo;ll have a look at optimising in the future.\nBearing in mind we are purely talking about single core processing. Splitting the load across the rest of the CPU cores, and spawning the data processes in parallel would\u0026rsquo;ve cut the time significantly. I\u0026rsquo;ll investigate this setup in the near future to optimise the compute power since Python is pretty slow in general.\n‚úàÔ∏è Data Animation This is the key component where we can physically see how the aircraft are moving over time.\nThis animation procedure is fairly straightforward when using ffmpeg in a bash script. We can point to a folder with a collection of images and ask the program to splice them together into a mp4 file (or whatever file type).\nAssuming the images are timestamped, the files should be sorted in chronological order so you don\u0026rsquo;t need to fiddle with passing additional sort arguments into your command.\nIt only takes a few minutes on my machine to splice together 850 images together.\nMy file structure is segmented into multiple folders partitioned by \u0026ldquo;date\u0026rdquo; and \u0026ldquo;visualisation type\u0026rdquo;, I simply pass those parameters into my bash script to generate the spliced video.\nThe script is only a few lines and looks like this:\n#!/bin/sh # $1 Specifies the Date of the folder to be accessed - E.g. \u0026#39;2024-08-24\u0026#39; # $2 Specifies the Image Folder To Be Accessed - E.g. \u0026#39;scatter\u0026#39; or \u0026#39;quiver\u0026#39; or \u0026#39;contourf\u0026#39; # Example of how to run bash script: $(./mkvideo \u0026#39;2024-08-24\u0026#39; \u0026#39;scatter\u0026#39;) base_folder=\u0026#34;$(dirname $(pwd))\u0026#34; output=\u0026#34;$base_folder/animate/videos\u0026#34; image_data=\u0026#34;$base_folder/data/get_states/$1/$2/crop\u0026#34; ffmpeg -framerate 24 -pattern_type glob -i \u0026#34;${image_data}/*.png\u0026#34; $output/$1_$2_movements.mp4 Other video processing was down on Kdenlive to add in the musical audio transitions.\nüìΩÔ∏è Data Time-lapse Videos illustrating the 16-hour scatter plot time-lapse of aircraft flying over North America. ","permalink":"https://filpill.github.io/projects/2024-05-21-flight-tracking/","tags":["üìä Data"],"title":"Flight Tracking Animations"},{"categories":null,"contents":"Summary This guide will quickly provide instructions on how to set up a self-hosted WireGuard VPN encrypted tunnel to access you home network. This will be especially handy if you are on a public Wi-Fi connection and need your data to remain secure.\nWe will also go through the steps of adding pi-hole to the device to include network wide adblock by blocking a list of domains which will try to render unnecessary advertising to us.\nFor this exercise I will be installing WireGuard onto a RaspberryPi, but you can do this ostensibly with any computer. I will assume that you already know how to SSH into your target device, because we are going to do this all on the command line.\nWhy Do I Need A Self-Hosted VPN? You will need a VPN if you are trying to access your home network/home devices from another location. The VPN will \u0026ldquo;tunnel\u0026rdquo; the connection to your home\u0026rsquo;s local network allowing you to access devices via SSH or RDP. You only need to expose a single port in your router that is accessible to the public.\nWireGuard - Server Host Visit PIVPN Website for WireGuard installation instructions - Has Installer Script For WireGuard or OpenVPN sudo apt-get update \u0026amp;\u0026amp; apt-get upgrade Use following command curl -L https://install.pivpn.io | bash Follow install instructions: Identify if DHCP Reservation is Enabled - DCHP is a protocol where each device connected on a local network is assigned a separate IP address. You will want to make sure that its enabled such that we can do the port forwarding for it. In the Router I later define a IP address reservation so I can maintain the same IP address for the RaspberryPi. Select static IP address - again, we will make an IP address reservation for the device (on the router). Choose WireGuard VPN Select user which will store WireGuard configurations Select DNS provider - I picked Quad9 DNS, but the choice is yours. Select the port RaspberryPi will be listening through (for UDP protocol/traffic) - By default its 51820 - it can be changed - but we need to open the port on the router firewall We are connecting via a public IP address which is fine since we have chosen a static IP Reboot pivvpn -a to add user (client machine aka pc or mobile define for example) This will generate a config file in /home/pi/config/\u0026lt;NAMEofCLIENTCONFIG.conf\u0026gt; You can copy this config to client by running the Secure Copy Protocol for example or any other method you like. scp raspberrypi@192.\u0026mdash;.\u0026mdash;.\u0026ndash;:/home/pi/config/\u0026lt;NAMEofCLIENTCONFIG.conf\u0026gt; /path/to/folder/in/client/machine You can also copy configurations with QR codes if you want. Router Firewall and Port Forwarding Router firewall procedures will be very specific to your model router and internet service provider. But we want to set the firewall rules and port to enable traffic to the VPN Server.\nIn your browser enter the IP address of the router i.e. the \u0026ldquo;Default Gateway\u0026rdquo; address. Go to LAN TCP/IP Setup and a \u0026ldquo;Address Reservation\u0026rdquo; - IP of device, Device Name and MAC address of device. The device in question could be one of your PC\u0026rsquo;s or mobile phones. Reboot Hub Create service type UDP that listens in on Port 51820:51820 (the raspberrypi vpn port we set earlier) Firewall rules add INBOUND firewall rule for the service we just created and add an action to ALLOW always and if you want you can specify applicable IP address that it can connect from. Apply the changes to the router. WireGuard - Client Machine Now we just need the configuration file that we copied earlier which contains keys (and config) to connect the client to the VPN server.\nDownload WireGuard for Windows Client and Install to Computer Take the client configuration and \u0026ldquo;Add Tunnel\u0026rdquo; \u0026ndash; The configuration contains the relevant keys for connecting to the VPN Server. The VPN tunnel will allow the client to encrypt the traffic and sent it to the Pi which then pipes into the router and thus keeping your data packets encrypted and secure. From a VPN perspective, that\u0026rsquo;s it, you should be able to connect to your home network externally. And since we\u0026rsquo;ve gone through the trouble of creating a VPN, you may as well enable adblocking in addition to a locally hosted recursive DNS Server.\nPi-hole - Adblocker Pihole will help prevent intrusive ads that try to target your device. This can be configure as a network-wide adblock capture to protect all devices.\nVisit Pi-hole Website for installation instructions. To install pihole:\ncurl -sSL https://install.pi-hole.net | bash Go through installation process and also ensure that you have a static IP address selected We will later create our own DNS Server using unbound after the installation Pihole will give you a password by default but you can change it with sudo pihole -a -p Go into the browser and login into pihole by accessing your Pi\u0026rsquo;s IP address via the browser: i.e. http://192.\u0026mdash;.\u0026ndash;.\u0026ndash;/admin Use you pihole credentials to login Unbound - Self Hosted Recursive DNS Server If you value your privacy, you may consider hosting your own DNS server. Why? Because your internet traffic gets routed to DNS servers such as CloudFlare or Google which are resolving your queries (even if you have a VPN tunnel enabled!).\nThe companies who own the DNS servers can see which IP addresses are querying which domains, so essentially they have a full view of your query history which all links back to your IP address.\nEven if you are \u0026ldquo;private browsing\u0026rdquo; all these queries are hitting their servers and its a mystery what DNS server providers are doing with your data:\nAre they collecting/storing it? Are they selling it to companies? Do they themselves \u0026ldquo;profile\u0026rdquo; individuals? Do they provide data to various government entities? How much should I trust these companies? At the end of the day, you should already know how valuable your data is, because all the advertising designed to manipulate you into a non-stop consumer.\nAnd here\u0026rsquo;s food for though, if this data is worth millions or billions, then we should not give it away for freely \u0026ndash; at least make them work for it\u0026hellip;\nAnyways, if you are still interested in setting up this DNS server, see below instructions:\nVisit Unbound Installation for installation instructions.\nTo install unbound:\nsudo apt-get install unbound Create file /etc/unbound/unbound.conf.d/pi-hole.conf Change the port number to 5335. Pihole is using port 53 so we dont want to share the same port, hence the usage of port 5335. Save and quit Restart the service by using command: sudo service unbound restart Check its working with command: dig pi-hole.net @127.0.0.1 -p 5335 DNS Settings on Pi-Hole Website Interface Go to the DNS settings and uncheck all the 3rd party Upstream DNS servers Add a custom IPv4 Address: 127.0.0.1#5335 Save and apply the changes DNS Settings on Client Machine We need to route the DNS requests via the RaspberryPi.\nAccess your clients network connections and remove all the other DNS values. Replace with preferred DNS IP address: aka. IP address of RaspberryPi \u0026ndash; This will now become your recursive DNS Server. Check your Pihole and you should see that queries are routing though the Pi as you use the internet and a percentage of them are getting blocked by the adblocker. ","permalink":"https://filpill.github.io/projects/2024-04-16-wireguard-vpn/","tags":["üíª Systems"],"title":"WireGuard VPN Server"},{"categories":null,"contents":"Summary This article goes through the process of incorporating Docker as a tool for creating a data ingestion system into a Postgres database. We will containerise both the database and the Python ingestion scripts.\nThe Github project this article is based on can be found here: github_repo: nba_stats\nRequired Containers We will be building 4 containers to manage the data ingestion:\nPostgres - Hosting a database Psql - Running Postgres CLI Tool (Another Postgres Instance) PgAdmin - Managing Postgres Python - Ingesting Data into Postgres DB The container set-up: graph LR; subgraph Stage 1: localhost - Prepare Data 0([API - data_extraction.py]) 1([Create data_ingestion.py]) cli0([Create ETL SQL Files]) 2([Postgres/PgAdmin docker-compose.yaml]) 3[Python Dockerfile] cli0--Copy .sql Files--\u003e cli1([psql\nDockerfile]) 0--Copy Data--\u003e3 1--Copy Script--\u003e3 end subgraph Stage 2: Docker Containers - Ingestion Pipeline subgraph Container Network 2--\u003eD0([compose Postgres DB]) 2--\u003eD1([compose PgAdmin]) 3--\u003eD2([build Python]) cli1--\u003ecli2([build psql]) end subgraph ETL/ELT D0--\u003eDB[(Database)] D1--Manage DB--\u003eDB D2--Ingest\nData--\u003eDB cli2--ETL via shell --\u003eDB end end subgraph Stage 3: Visualisation DB--Live\nQuery--\u003eviz[Connect DB to Visualisation Tool] end Stage 1 - Prepare Raw Data Retrieving data from an open-source API providing NBA data: balldontlie.io. Python scripts to request JSON data from all endpoints. Merge JSON data into a consolidated file for each endpoint. Stage 2 - Create Dockerised Data Ingestion Pipeline Create Dockerised instance of Postgres database. Create Dockerised instance of PgAdmin to connect and manage database. Create tables into Postgres DB using Dockerised Python ingestion script. Use SQL to transform data and create tables for analytical capability. Automate the ETL procedure by running SQL scripts with psql. Stage 3 - Analyse and Visualise Data Connect dashboarding tool to database and showcase data visualisations. Why Docker? What\u0026rsquo;s the purpose of containerising Python and Postgres?\nOne of Dockers greatest strengths is the ability to standardise a software environment in order to run a collection of applications:\nIt\u0026rsquo;s much more efficient than creating an entire Virtual Machine in order to replicate a \u0026ldquo;standardised environment\u0026rdquo;. Docker containers share its resources natively with the host, which means your application only uses the compute it needs. The container build files can be deployed easily to any computer/cloud/server and it will run identically on all instances. A virtual machine has to reserve a portion of system resources such as the Memory and Drive space. Virtual machines are not scalable resources at runtime. This limits how many instance you can spin up on a given computer/server.\nTo sum up: containerisation provides easier code deployment/distribution, and can be scaled more easily in conjunction with cloud technology.\nData Ingestion Pipeline Building Postgres and PgAdmin Containers Simultaneously Docker has a utility called docker-compose which provides the capability of creating multiple services simultaneously from a .yaml file. All images are created simultaneously.\nA natural benefit of this utility is that docker automatically sets up a default networking configuration for containers to communicate with each other in the same config.\nI\u0026rsquo;ve decided to run Postgres and PgAdmin in the same configuration file since it would be natural to be natural to pair the database with DBMS.\nservices: pgdatabase: image: postgres:13 environment: - POSTGRES_USER=root - POSTGRES_PASSWORD=root - POSTGRES_DB=nba volumes: - \u0026#34;./nba_postgres_data:/var/lib/postgresql/data:rw\u0026#34; ports: - \u0026#34;5432:5432\u0026#34; pgadmin: image: dpage/pgadmin4 environment: - PGADMIN_DEFAULT_EMAIL=admin@admin.com - PGADMIN_DEFAULT_PASSWORD=root ports: - \u0026#34;8080:80\u0026#34; This file must be called docker-compose.yaml in order to be able to run the command: docker compose up.\nThis will instantiate those two services under the same network so they are able to communicate with each other.\nBuilding Container for Data Ingestion Script For the purposes of my system, I have decided to keep my ingestion script separate from my other 2 containers. I don\u0026rsquo;t want to invoke the ingestion process every time I run the database. This process I\u0026rsquo;ve kept manual for now.\nWe can create an independent Docker image and connect via the default network generated by the \u0026ldquo;docker-compose\u0026rdquo; tool.\nTo create a singular docker image, you need a Dockerfile. For this project, it looks like this:\nFROM python:3.9 RUN apt-get install wget RUN pip install pandas sqlalchemy psycopg2 WORKDIR /app COPY data_ingestion.py data_ingestion.py COPY data/combined data ENTRYPOINT [ \u0026#34;python\u0026#34;, \u0026#34;data_ingestion.py\u0026#34; ] In this Dockerfile I am specifying:\nThe software versions and packages. The working directory. Local files/scripts to be copied. Entering program directly through Python. To build the image from the Dockerfile, you will need to run the following Docker command:\nNote: If you are running docker from Windows, you must prefix docker commands with \u0026ldquo;winpty\u0026rdquo;\ndocker build -t nba_ingest:v001 . The build command has generated an image called nba_ingest:v001.\nHowever to run the image in a container, we can use the docker run command as follows.\nThe command is structured in a manner to pass to docker arguments to the docker tool and the python arguments to the image containing the python script.\ndocker run -it \\ --name=pyingest \\ --network=docker_sql_default \\ nba_ingest:v001 \\ --user=root \\ --password=root \\ --host=pgdatabase \\ --port=5432 \\ --db=nba \\ As per the command, we have named this container pyingest. Which means we cannot run the same run command again.\nTherefore, if the container stops and you want to run the program again; you can simply use the command:\ndocker start -i pyingest Container Summary After this entire process, you should 3 containers that look like this:\nNetworking Explanation In this specific scenario we are working in, we do not need to manually create a network as mentioned earlier.\nThis is automatically defined by the docker-compose process we ran earlier. We can simply borrow the default network name in order to connect this container to the database.\nThe default name of the network from docker-compose is the working directory of the .yaml file suffixed with \u0026quot;_default\u0026quot;.\nYou can verify this by running the command:\ndocker network ls Since our working directory is docker_sql, the network created earlier is called \u0026ldquo;docker_sql_default\u0026rdquo;. We have simply applied this to the network argument in our docker run command to allow the connection between those containers.\nPgAdmin - Data Transformation From this point forward, you will have a data ingestion pipeline running, all you need to do is connect to the DBMS to enact transformations on the data we have ingested.\nWe can access PgAdmin in our browser via the port number we have designated: localhost:8080\nAutomated Docker Container/Image Rebuilding You are in the process of developing or changing containers configurations, you will likely encounter the issue of having to delete the containers/images and re-build them. If you are doing this for isolated containers, it can become very annoying and repetitive.\nTo avoid running the docker rm and build commands constantly, consider building a script to automate that process to speed up your config changes:\n#!/bin/bash container=\u0026#34;psql_contain\u0026#34; image=\u0026#34;psql:v001\u0026#34; # Re-Build Container with New Configuration function update_container { winpty docker rm $container winpty docker rmi $image winpty docker build -t $image . winpty docker run -it \\ --name=$container \\ --network=docker_sql_default \\ $image } # Run Dockerised Python Ingestion Script function run_script { winpty docker start -i $container } declare -A container_options=( [1]=\u0026#34;1 - Update Container/Image with New Configuration\u0026#34; [2]=\u0026#34;2 - Run psql\u0026#34; ) keys_sorted=($(echo ${!container_options[@]} | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | sort -n)) while true; do echo \u0026#34;==============================================\u0026#34; echo \u0026#34;Please Select An Action (Enter Integer Value):\u0026#34; echo \u0026#34;==============================================\u0026#34; for key in \u0026#34;${keys_sorted[@]}\u0026#34;; do echo \u0026#34; ${container_options[$key]}\u0026#34; done read num case $num in 1) update_container ;; 2) run_script ;; *) clear echo \u0026#34;-------------------------------------------------\u0026#34; echo \u0026#34;--- Invalid Selection - Enter Value on List ---\u0026#34; echo \u0026#34;------------------------------------------------- \u0026#34; continue ;; esac break done This script will give you two options:\nSelecting Option 1 - If you need to re-image the container because you updated your scripts or have new data to be copied into the container. Selecting Option 2 - Will simply run the existing container in its current state. I\u0026rsquo;ve made multiple instances of this scripts for each individual image/container pair. But it\u0026rsquo;s a little redundant given the duplicity. It would be wiser to pass in parameters for an image name and container name via some config or secondary script. I\u0026rsquo;ll rework this concept/soltion in the future.\nBut my main point here is that we want to avoid manually typing out Docker commands. Once we internalise what each command does, its much better to automate that process.\nAutomating the ETL with psql Ideally we don\u0026rsquo;t want to manually run an ETL process every day via the graphical interface. We want to free up our time to engage in other work in parallel. Therefore, I would strongly suggest the incorporation of scripting out the transformation process.\nThankfully, Postgres comes pre-packaged with psql which is a command-line tool to interface with the database. Using this tool we are able to execute a series of SQL scripts to transform the data to our requirements in the destination tables.\nEven though Postgres, comes with psql, I decided to load it into a separate container strictly for the purpose of ETL which is arguably a more bloated way of dealing with the situation, (but we\u0026rsquo;ll run with it for the sake of having a functional system).\nFor purposes of running it independently from the \u0026ldquo;docker-composition\u0026rdquo;, this program will be in it\u0026rsquo;s own container with an independent Dockerfile:\nFROM postgres:13 WORKDIR /app COPY tf tf COPY etl.sh etl.sh ENTRYPOINT [ \u0026#34;bash\u0026#34; ] RUN chmod +x etl.sh CMD [ \u0026#34;./etl.sh\u0026#34; ] We will be entering this container via bash and automatically executing our etl.sh script. The script points to all the SQL files we have saved in the folder named tf.\nUnder the hood of the etl.sh script is simply a one-liner psql command which after logging into the relevant database, runs all the SQL files I have specified explicitly:\n#!/bin/bash psql postgresql://root:root@pgdatabase:5432/nba -f tf/create_tf_season_averages.sql -f create_tf_unioned_games.sql You can have as many or as little SQL scripts as you need.\nI haven\u0026rsquo;t set it up yet but assuming you have a server arrangement, you can schedule to run this Docker container using a crontab (daily, weekly, monthly etc). And you can easily run the psql ETL container without your intervention.\nConclusion Hopefully this article illustrates the power and utility of Docker as a tool.You should be able to build a rudimentary database system which can easily be deployed on a server or cloud provider.\nAlthough further care should be considered with the passwords and how to protect them as they are visibly bundled in all the scripts. So, consider how to incorporate password encryption and improve security for this set up.\nI will follow up in a later article when I get a chance to process and analyse all the NBA data from the API. I will likely try to visualise the NBA dataset using streamlit hosted on a Raspberry Pi.\n","permalink":"https://filpill.github.io/projects/2024-01-16-docker-ingestion/","tags":["üíª Systems"],"title":"Dockerised Postgres Database and Ingestion"},{"categories":null,"contents":"Introduction After installing a fresh copy of Arch Linux, you are probably wanting to get around to setting up some type of graphical environment.\nIt can be difficult to setup a graphical environment for the first time if there is not much guidance. Linux advice can be very scattershot, and can leave people feeling quite lost. You will end up digging through heaps of internet archives just to surface something vaguely relevant to your own setup.\nThis article should guide you on creating a basic graphical setup for your Linux machine relatively quickly and with ease.\nNote: Depending on your own software and hardware, you may need to deviate from these prescribed procedures from time to time. Read the documentation for the programs you use and adjust accordingly.\nI will prescribe the software choices in this article, however, once you feel comfortable, you can choose to go in your own direction.\nI setting up a window manger as the foundation for this guide. My system is running Arch Linux running under Hyper-V.\nCreating a User - Getting Sudo Permissions At this stage, you probably only have a root user on a fresh install, so it would be a good idea to create a user for yourself:\nuseradd -mg wheel filpill You want to add yourself to the wheel group as we will make some changes to enable you to have access to the sudo command i.e. allow admin privileges.\nAdd a password for that your using this command (and follow the prompts):\npasswd filpill Next you will need to edit the sudoers file in order to give the wheel group the relevant access.\nYou can find the file at /etc/sudoers.d and you will need to uncomment the following line:\n%wheel ALL=(ALL) ALL Now you will be able to execute commands as root after being prompted for a password.\nX11 - Display Server Since we are using a window manager as the graphical environment, we are going to need to have some kind of display server.\nTraditionally for Linux, Xorg/X11 is the display server of choice as it has the best compatibility with other software. Alternatively, you have Wayland as a more efficient option, although you may run into compatibility issues depending on what software is running.\nIn order to maximise compatibility and stability, I will continue to configure using Xorg for the time being until Wayland catches up.\nThe following programs will need to be installed:\npicom xorg xorg-server xorg-xinit xf86-video-fbdev (video driver for Virtual Machine) If you are installing on bare metal, then you will probably require a different video driver, so just research the one you need depending on the hardware you have.\nUsing pacman, you can install with is command:\nsudo pacman -S picom xorg xorg-server xorg-init xf86-video-fbdev dwm - Window Manager Dwm is a Suckless software and will be forming the backbone for the environment we are building.\nHowever, the install procedure and configuration is a little different compared to other programs.\nWe cannot run a simple pacman command to install it. The program is written in C and needs to be compiled directly from those C files.\nYou will need to visit the Suckless Website to get the source files and relevant patches.\nThey have the base version of dwm on git, so you will need to run a git clone command to pull it down onto your machine. Run the git command:\ngit clone git://git.suckless.org/dwm If you need to change the configuration, you can do all the changes inside the file **config.h\u0026quot;.\nTo compile the dwm configuration:\nsudo make clean install It\u0026rsquo;s fairly straightforward, if no errors occur, you have successfully installed dwm.\nHowever, if you ever need to change the config, you will need to re-compile those C-files.\nIt would also be advisable to host a copy of your own dwm configuration on your own git repository so that you are able to deploy onto any machine very easily.\nst - Terminal Emulator There are many terminal emulators you can use as an option to supplement dwm.\nHowever, for continuity I would go for st which is also a Suckless utility and follows a similar procedure.\nClone from the suckless website:\ngit clone git://git.suckless.org/st Configuration changes occur in the file called config.h.\nCompiling st into your desktop is done via this command:\nsudo make clean install Exact same procedure as dwm. Again, you can save the st configuration onto your own git repository.\nConfig Files - .xinitrc and .bash_profile .xinitrc Your .xinitrc is where you will:\nStart the terminal compositor as a background process Start dwm After vimming into ~/.xinitrc; add these two lines in:\npicom -b exec dwm Note: Exec dwm MUST be the last executed command of the xinitrc file\n.bash_profile If you are using bash, this file will be called .bash_profile.\nDepending on which shell you are using, the filename may be different. E.g. for zsh, the corresponding file would be called .zprofile\nWhat you need to do in this file is essentially add a command to start the display server. The command for starting the display server is exec startx.\nAfter adding that command, your .bash_profile would look something like this:\n# # ~/.bash_profile # [[ -f ~/.bashrc ]] \u0026amp;\u0026amp; . ~/.bashrc [[ $(fgconsole 2\u0026gt;/dev/null) == 1 ]] \u0026amp;\u0026amp; exec startx --vt1 After saving your .bash_profile, you should be able to reboot your system.\nThe next time you login into your user profile, it should execute the commands in both ~/.bash_profile and ~/.xinitrc in order to start the display server and start dwm.\nNow you should have dwm automatically loaded in and you should be able to spawn st terminal\u0026rsquo;s inside dwm.\nlf - Terminal File Explorer The file explorer I recommend using is called lf. It\u0026rsquo;s handy for exploring the terminal more efficiently. Also very useful for bulk operations.\nYou can install using pacman:\nsudo pacman -S lf To open lf, simply type: lf into your terminal window.\nExtending lf I also recommend extending this program to get some more functionality from it by:\nAdding icons to the file explorer Adding previews for text, images and pdf\u0026rsquo;s Icons - lf In order to get the icons to work for the lf configuration, you will need two things:\nCreate an icons file in the lf config Install a \u0026ldquo;nerdfont\u0026rdquo; You can make the icons file inside ~/.config/lf with:\ntouch ~/.config/lf/icons Go to the Gokcehan - lf git repo - icons.example, copy the entire contents and paste directly into your newly created icons file.\nYou will notice that you terminal is unable to read the iconography. This is solved by installing a \u0026ldquo;nerdfont\u0026rdquo;.\nGo to Nerd Fonts Website and download any font of your choosing.\nUnzip the downloaded font and paste the contents into ~/.local/share/fonts.\nIn order for st to change the font being used, you must edit the config.h of st.\nThere should be a line in config.h which defines the font name, simply edit the line with the new name. For example my st font is configured like this:\nstatic char *font = \u0026#34;GeistMonoNerdFont-Medium:pixelsize=14:antialias=true:autohint=true\u0026#34;; Run sudo make clean install to re-compile st and you should now be able to read those characters in the icons file.\nThe next time you run lf on a fresh instance of st, you should be able to see icons of the various files in your system.\nIt should feel much easier to navigate more easily!\nAs a quick tip, you can press zh when inside lf to toggle visibility of hidden files. You will likely want to do this if you are searching for dotfiles.\nImage Preview - ctpv ctpv is program made by NikitaIvanovV and is designed to be integrated directly into lf to avoid the use of wrapper scripts.\nWe will only need to make some minor additions to our lfrc.\nTo install ctpv, run these commands:\ngit clone https://github.com/NikitaIvanovV/ctpv cd ctpv make sudo make install Go to your lfrc located in ~.config/lf/lfrc and add these lines:\nset previewer ctpv set cleaner ctpvclear \u0026amp;ctpv -s $id \u0026amp;ctpvquit $id These configuration lines effectively work to bring up the image previews and (clearing in prep for next image).\nThat\u0026rsquo;s all you need; the preview should accommodate most file types:\nzsh - Changing default Shell I prefer to use zsh as the default shell as opposed to bash. There are some extra features which can enhance the experience e.g.:\nBetter Autocomplete Ability to Tab through Change directories without typing \u0026lsquo;cd\u0026rsquo; This is by no means an exhaustive list. You can configure your shell to have as many or as little features as you want.\nYou can install zsh using the pacman command:\nsudo pacman -S zsh The first time you run zsh (by typing zsh into the terminal), you will be greeted with CLI configuration menu to perform a first time set up. Once that\u0026rsquo;s complete, the setup menu will not appear again, it will just launch zsh.\nHowever, if you want to change your default shell from bash to zsh when you login, you will need to run some extra commands.\nFirst you will need to list all the available shells installed on the machine:\nchsh -l After finding the one you wish to use by default, you can type the command:\nchsh -s /bin/zsh You will be prompted for your password and after this stage the default shell is now zsh. The next time you login, it will spawn a zsh shell within st.\nDon\u0026rsquo;t forget to migrate all the configuration you have down in ~/.bash_profile to ~/.zprofile as bash and zsh don\u0026rsquo;t share the same config files.\nAdding Terminal Based Copy-Paste Bindings This is for enabling the clipboard across any program or instance.\nYou can add a couple of lines to your neovim config in ~/.config/nvim/init.vim:\n\u0026#34;Copy paste to X11 Clipboard vmap \u0026lt;leader\u0026gt;\u0026lt;F6\u0026gt; :!xclip -f -sel clip\u0026lt;CR\u0026gt; map \u0026lt;leader\u0026gt;\u0026lt;F7\u0026gt; mz:-1r !xclip -o -sel clip\u0026lt;CR\u0026gt; These key bindings xclip commands allow terminal based copy-pasting:\nLeader-F6 for Copying Leader-F7 for Pasting You will need to go into \u0026ldquo;visual mode\u0026rdquo; in order to enact the copy command and you can paste into another terminal using normal mode.\nNote for Copying from Browser to Terminal: Highlight what you are copying and paste by clicking the middle mouse button on the target.\nAdding a wallpaper You need the xwallpaper program to run a command to change the wallpaper:\nsudo pacman -S xwallpaper Add a line inside your ~/.xinitrc file to apply a chosen wallpaper every time your computer logs on:\nxwallpaper --output Virtual-1 --stretch ~/desktop_setup/wallpaper/cartoon_cove.jpg You would need to adjust the arguments to output to the correct display and path to your chosen image.\nConclusion At this stage you should have a fairly rudimentary setup, however, it should get you acquainted with the various features Linux provides and how to approach the configuration in the future.\nYou should consider saving all your configurations onto a git repo so you can more easily manage your setups and deploy a desktop environment more easily in the future.\nIdeally, you would want to run some of these procedures using shell scripts. Applying some automation will help in getting an environment setup more readily and consistently.\nFrom this foundation, I recommend spending time experimenting with different programs to develop workflows that fit your specific needs.\n","permalink":"https://filpill.github.io/projects/2023-11-14-linux-gui-setup/","tags":["üíª Systems"],"title":"Setting Up Linux Desktop Environment"},{"categories":null,"contents":"Why Share Files Directly Between Windows and Linux? I\u0026rsquo;ll share my reasons why here:\nI have 2 separate work streams for windows and linux respectively on my PC: Certain tasks are more optimal to do on windows e.g. CAD, video/photo editing etc. Other tasks are more optimally accomplished on linux software: file management, code development etc. The workflow eventually bleeds over from windows to linux (or vice versa) and I need file transfers to complete my work e.g. adding images to this website. Many of the images hosted on this website were created on a windows PC. And I painstakingly used Google Drive, as the method of file transfer.\nThis process was so laborious that it was enough to motivate me to make a better workflow. Especially when dealing with large volumes of files for every article written.\nI would do \u0026ldquo;everything\u0026rdquo; in Linux if I could, but sadly some things just cannot replicate the experience on Windows. The best illustration of this is probable CAD; an engineering industry that was literally built on the foundation of the Windows OS. Sadly, there is no Linux equivalent that can do better in my experience. Not even close.\nI will show you how to create a shared folder, if you end up in such a position as myself.\nIn the instance of this article I am running the following setup, but will likely work with any Linux distribution:\nArch Linux - Hyper-V VM Create Windows Shared Folder On windows, you need to decide where you will keep that shared folder, you can make it any given directory. For illustrative purposes, I\u0026rsquo;ve called mine \u0026ldquo;vm_share\u0026rdquo;.\nYou can right click and going into the properties of \u0026ldquo;vm_share\u0026rdquo; folder. Click on the Sharing Tab and click the Share button. You will need to share with your own Windows User.\nThen on Advanced Sharing, you will need to check \u0026ldquo;Share this folder\u0026rdquo; checkbox and set the permission of the folder. I\u0026rsquo;ve current set mine to Read and Change.\nBefore you completely exit out of the properties, take note of the network path. For me it looked like this:\n\\\\DESKTOP-R1B3P7P\\vm_share_folder Find Default Switch IP Host PC/VM Now you need to find he IP address of the Windows host in order for the Linux VM to connect (mount file systems).\nYou will need to find the name of your PC \u0026ndash; mine is : DESKTOP-R1B3P7P\nWith the later revisions of Hyper-V, you will probably not need to configure any Virtual Switches, it should be working out of the box with \u0026ldquo;Default Switch\u0026rdquo; in my experience.\nYou may want to create variable (for the later bash script) which dynamically grabs this IP should it change in the future. The variable could be equated to this bash command:\nnmblookup DESKTOP-R1B3P7P | head -n 1 | cut -d \u0026#39; \u0026#39; -f 1 Here of course you can just substitute for your own PC name.\nCreate Mount Point I haven\u0026rsquo;t tested extensively, but I\u0026rsquo;m assuming you can mount the shared folder onto any directory you wish.\nAt the time of writing, I decided to create mine in /mnt/Hyper-V like so:\nmkdir /mnt/Hyper-V CIFS - Common Internet File System - Install Program: cifs-utils Microsoft developed their own system called CIFS (Common Internet File System) and it is the client for their file management. Therefore, you will need to download cifs-utils for Linux to work with that system. You can download like so on Arch:\nsudo pacman -S cifs-utils Edit FS Tab In order to have a mountable folder on Linux, you would need to specify this in your /etc/fstab file.\nIn our case we would like to mount onto /mnt/Hyper-V\nRun Sudo vim /etc/fstab to edit the file and add the following line to the bottom (underneath my existing partition mount points):\n# Hyper-V //DESKTOP-R1B3P7P/vm_share_folder /mnt/Hyper-V cifs username=Filip,domain=sealab,noauto,rw,users 0 0 Replace with desktop name,folder,usernames with your own set-up.\nNote the slashes must be in the forwards orientation to conform to the linux file system.\nWrite Bash Script to Mount Windows Shared Folder to Linux File System Now you can put the mounting commands into a script such that you don\u0026rsquo;t always need to type the full command for the mounting procedure. It could also be stuffed into some kind of systemD process for it auto-run when you boot up (if you want). You can name the script whatever you want and place wherever you want.\nmount -t cifs //DESKTOP-R1B3P7P/vm_share_folder /mnt/Hyper-V -o , ip=\u0026lt;Enter your Host PC IP Address e.g. 152.41.97.1\u0026gt; Reminder that you Ip address can be bundled into a variable using \u0026mdash;-\u0026gt; nmblookup DESKTOP-R1B3P7P | head -n 1 | cut -d \u0026rsquo; \u0026rsquo; -f 1\nA potential mounting script can look like this, but you can customise to your liking:\n#!/bin/sh desktop=\u0026#34;DESKTOP-R1B3P7P\u0026#34; windows_folder=\u0026#34;vm_share_folder\u0026#34; linux_folder=\u0026#34;/mnt/Hyper-V\u0026#34; desktop_ip=$(nmblookup $desktop | head -n 1 | cut -d \u0026#39; \u0026#39; -f 1) mount -t cifs //$desktop/$windows_folder $linux_folder -o,ip=$desktop_ip I\u0026rsquo;ve only extracted the variables to help make the mount command appear more readable. And also the IP address for the desktop will be pulled out dynamically with the nmblookup command.\nExecute Bash Script After executing the script, you will be prompted for the windows pwd which can enter into the terminal. And if there are no errors, you should be able to access vm_shared (Windows) from /mnt/Hyper-V (Linux).\nHopefully the article assists in helping you get a more seamless file transfer experience between Windows and Linux.\n","permalink":"https://filpill.github.io/projects/2023-11-13-windows-linux-fileshare/","tags":["üíª Systems"],"title":"Windows to Linux VM Fileshare"},{"categories":null,"contents":"Summary I\u0026rsquo;ve designed out a \u0026ldquo;cyber deck\u0026rdquo;/frame for my Raspberry Pi to convert it into a usable personal computer. It features a 7 inch touch display and a web camera plugged into the boards DSI (Display Serial Interface) connectors. Should the design become more compact in the future, the ribbon cables are ideal for routing inside tight enclosures.\nThe only necessary wired connection is a USB-C to provide power to the board. I\u0026rsquo;ve got my keyboard and mouse connected wirelessly via Bluetooth to maintain control of the Raspberry Pi. This avoids having to constantly plug devices in and out between the Pi and my main PC.\nIn order to regulate the temperature of the Pi, I\u0026rsquo;ve mounted a 3V fan onto the PCB to cool the Pi\u0026rsquo;s CPU and memory chips. This is to maintain the peak performance of the chips in question.\nThe open back design also enables easy access to the GPIO headers should you need to plug in and program your electronics. I\u0026rsquo;ll hook up some servo\u0026rsquo;s and play around with them with the Pi using Python.\nRaspberry Pi Assembly ","permalink":"https://filpill.github.io/projects/2023-07-10-cyber-deck-design/","tags":["‚öôÔ∏è Engineering"],"title":"Cyber Deck Assembly - Raspberry Pi"},{"categories":null,"contents":"Summary This article documents how to remotely access your Raspberry Pi using either Secure Shell (SSH) or Remote Desktop Protocol (RDP).\nThe benefit of remote access is that you do not need to constantly plug in a wired keyboard and mouse to do some configuration on your Pi devices.\nThe SSH access is ideal for cases with exclusive involvement of the terminal. The remote desktop access is required for situations using the graphical desktop environment.\nBluetooth Setup If you want to have reliable bluetooth when powering the raspberry pi, you will need to ensure that you \u0026ldquo;trust\u0026rdquo; the device after pairing with it. See the details listed below.\nYou can access the bluethooth cli by typing \u0026ldquo;bluetoohctl\u0026rdquo;\nThe most useful commands are:\npower on - turns on bluetooth system pairable on - sets controller to be in pairable mode scan on - starts scanning for device mac addresses in the bluetooth range/vicinity pair - attempts to pair the device e.g. keyboard trust - will remember the paired device and will auto connect to device next time its powered on exit - quits cli tool IP Address Something that\u0026rsquo;s common to both processes is that they both require the IP Address of the device.\nTo get the IP address, you can type this command into the Raspberry Pi terminal:\nhostname -I # or ping raspberrypi.local # or ifconfig Secure Shell (SSH) Remote Access For the SSH Remote Access, I will be connecting to my Raspberry Pi via my Arch Linux VM which is running a zsh terminal. But you can follow this process on any shell, it will be exactly the same.\nPre-requisites Set up wireless networking capability onto the boot disc. Obtain Raspberry Pi\u0026rsquo;s IP Address Create hostname for IP in the /etc/hosts file Ensure Raspberry Pi \u0026ldquo;Server\u0026rdquo; has OpenSSH Daemon running as Service Setting Up Networking Assuming you are running a minimal distribution such as Raspberry Pi Lite OS, you will need a way to get access to the internet.\nGet the SD from the Pi which has the image installed onto it, and read it from your main PC.\nThe following steps will be performed in git bash, but you can also do this in your file explorer:\nNavigate to the boot device volume and create an ssh file:\ntouch ssh In the same directory (boot), we need to make a wpa_supplicant.conf file. This will provide the configuration to enable you to connect your wireless network setup.\nvim wpa_supplicant.conf Then enter the following into the configuration, and substitute the placeholders with your own network SSID and password to access the Wi-Fi.\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 country=\u0026lt;Insert 2 letter ISO 3166-1 country code here\u0026gt; network={ ssid=\u0026#34;WIFI-NAME\u0026#34; psk=\u0026#34;WIFI-PASSWORD\u0026#34; } You can safely remove the SD card from your PC.\nPut the SD Card back into the Pi and power it on.\nEnsuring OpenSSH Daemon is running (For Linux server) For Raspberry Pi I\u0026rsquo;m assuming this is running by default, however, for other Linux systems, you will have to ensure that it is installed AND enabled as a service. Otherwise you will encounter a lot of confusion when your SSH requests are not being accepted:\nIf openssh is not installed on your server, then install using your relevant package manager e.g.: sudo pacman -S openssh To check the status of the daemon: systemctl status sshd If it disabled then start the service systemctl start sshd By default it will be listening for TCP connections on port 22 SSH Into Raspberry Pi Via Terminal From the terminal of the machine you are trying to attempt the remote access, you can SSH using this command:\nssh \u0026lt;USERNAME\u0026gt;@\u0026lt;IP_ADDRESS\u0026gt; If you didn\u0026rsquo;t configure your pi user credentials, for reference these are the defaults:\nusername: pi password: raspberry So the SSH command may look like this for example:\nssh pi@192.---.-.--- You would then be prompted with a password for which you will have to enter in order to gain access.\nBut for security purposes I recommend you make your own username and password, then substitute with your own credentials.\nThe userame on the Raspberry Pi is filpill, so it would be modified like this:\nssh filpill@192.---.-.--- Replacing IP Address with Hostname Now obviously its not that easy to remember an IP address every time you want to SSH into the Pi. So we could store an alias for the IP in the /etc/hosts directory.\nsudo vim /etc/hosts You\u0026rsquo;ll want to add a line in this file with the IP Address and hostname of the Raspberry Pi. The line you will add will look similar to this:\n192.---.-.--- raspberrypi So the next time you want to SSH into the Pi, instead of writing the full IP address, you can just write this:\nssh filpill@raspberrypi The command functions all the same in addition to being much easier to remember into the future.\nSSH Keys - Password-less Access In order to increase the security of the SSH protocol, its recommended to create a set of SSH keys.\nYou can simply create a key-pair in your shell with this command:\nssh-keygen Then you will want to get the public key onto the raspberry pi to enable the key pair interaction. And it\u0026rsquo;s similar to SSH\u0026rsquo;ing into your device.\nssh-copy-id filpill@raspberrypi This will copy the public key into your pi, so the next time you try to SSH into the Pi, it will automatically unlock using the private key on your computer.\nRemote Desktop Access via Windows For demonstation purposes I will be using the Windows 10 Remote Desktop Connection application to provide my access. The same concepts can be applied to alternative software.\nPre-requisites There are a few of pre-requisite tasks to complete before attempting remote access:\nInstall a Graphical OS environment for the Pi Obtain the Pi\u0026rsquo;s IP Address Install xrdp on the Raspberry Pi Add a secondary user to the Raspberry Pi with Sudo privillages Enable Remote Desktop Access in the raspi-config Install xrdp To install xrdp, type this command into the Raspberry Pi terminal:\nsudo apt-get install xrdp Then you need to start the xrdp service like this:\nsudo systemctl start xrdp New/Secondary User To add a new user, type this command:\nsudo adduser \u0026lt;INSERT USERNAME\u0026gt; Skip all the prompts to fill in user details with the Enter Key.\nThen to give the user sudo privileges i.e. adding them to the sudo group:\nsudo adduser \u0026lt;INSERT USERNAME\u0026gt; sudo Enable Remote Desktop Access To enable this access, you need to enter the command:\nsudo raspi-config You need to navigate it the interface options; you need to enable the graphical remote access on the VNC option.\nThen finish the config to apply the settings and reset the Raspberry Pi.\nWindows - Remote Desktop Connection Application Now you can switch back to the Windows PC environment.\nOpen the Remote Desktop Connection App.\nEnter the IP Address you noted earlier from the Raspberry Pi and proceed to connect.\nYou should find yourself inside a login screen for xdrp.\nFor the user credentials, you should use the username and password you created on the Raspberry Pi as the secondary user.\nYou should now have graphical access to your Raspberry Pi via your Windows Machine.\nNote: This RDP procedure only works in a graphical OS, this does not apply to minimalist operating systems without a graphical UI because the display server will have nothing to render.\n","permalink":"https://filpill.github.io/projects/2023-07-06-raspberry-pi-ssh/","tags":["üíª Systems"],"title":"Raspberry Pi Remote Access Guide"},{"categories":null,"contents":"Summary This project has been made to practise the use of sending instructions to various electronics for both the Arduino and the Raspberry Pi.\nI\u0026rsquo;ve segmented this project into two parts/versions:\nArduino - Mouse Control via Serial Communications Port Raspberry Pi - Mouse Control via Python Tkinter Program Arduino The arduino project is very simple, there is a program written in Processing which takes the mouse inputs send strings (enclosed in \u0026lt;\u0026gt;) to the Arduino via serial communications.\nA small delay of 100ms is maintained to limit the rate of data coming to the Arduino board. The Arduino has a small serial buffer, it can only read 8 bytes at a time. Therefore we need to limit the rate of data to prevent a serial buffer overflow. If this issue occurs, then the servo will not be able to effectively read the data stream.\nThe Arduino has a sketch uploaded to the board that will do a number operations.\nIt will ingest the string data character by character from \u0026lsquo;\u0026lt;\u0026rsquo; and terminate at \u0026lsquo;\u0026gt;\u0026rsquo;. It will concatenate all the characters into a single string. Split the string to serve as inputs to its respective components i.e. Servo1 and Servo2 The inputs will be converted to integers to map the angle for the Servos. Actuate the Servos graph TD; subgraph Processing Program P0([Start Processing GUI Program])--\u003e P1[Mouse Inputs -- Pitch/Yaw] P1 --\u003e P2[Formulate Data String \u003c000000\u003e and send over COM Port] end subgraph Arduino Program P2--\u003eA0[Read Data Stream] A0--\u003eA1[Start Ingesting the data after reading '\u003c' character] A1--\u003eA2[Build string by ingesting characters one at a time] A2--\u003eA3[Terminate the string when reading '\u003e'] A3--\u003eA4[Split string into respective components to prep servo inputs] A4--\u003eA5[Cast servo inputs to integers and map servo angles] A5--\u003eA0 end subgraph Arduino Program A5--\u003eS1([Actuate Yaw Servo]) A5--\u003eS2([Actuate Pitch Servo]) end Raspberry Pi The Raspberry Pi project is using a Python script to drive the inputs. I\u0026rsquo;ve created a Tkinter program which maps the mouse position on the GUI to a pair of servo angles for the turret to move.\nThe mouse movements are passed into an event handler to locate mouse position on the window x,y coordinates. We use positional data on the mouse to calculate a servo angle based on the coordinate on the drawn window.\nIn order to actuate the servos, the angle inputs must be mapped to the corresponding duty cycle. The duty cycle can take any range of values between 2 and 12 for this particular servo. It handles angles in the range between 0 and 180 degrees.\nThe duty cycle can be calculated like with this calculation:\n1/18 * angle + 2 Then we send the pair of updated duty cycle values to the respective GPIO pins in order to drive the servo positions.\ngraph TD; subgraph Python Program P0([Run Python])--\u003e P1([Start Tkinter GUI]) P0 --Initialise--\u003e G0[GPIO Pins] P1 --\u003e P2[Mouse Movements Passed to Event Handler] P2 --\u003e P3[Convert X,Y Coordinates to Servo Angles] P3 --\u003e P4[Map Servo Angles to Duty Cycle Values] P4 --\u003e P5[Send Duty Cycle Values Through GPIO Pins] P5 --\u003e G0 G0 --\u003e S1([Actuate Yaw Servo]) G0 --\u003e S2([Actuate Pitch Servo]) end ","permalink":"https://filpill.github.io/projects/2023-07-03-servo-turret/","tags":["‚öôÔ∏è Engineering"],"title":"Servo Turret Control"},{"categories":null,"contents":"Summary Finished designing and building a functional 3D Printed RC car which accidentally ended up resembling a tractor by chance. That\u0026rsquo;s the consequence of a lack of planning and design constraints\u0026hellip;\nThe reason for the large wheels in the back is because of how I started my design process. The design was built around the gearbox. Since the gearbox ended up being very tall, I figured I would just mount some very large wheels to clear the ground. Therefore I didn\u0026rsquo;t need to think about mounting points for the rear wheel, nor a CV jointed drive shaft. It was simply direct drive and it was all 3D printed (except the steering mechanism).\nAfter doing some testing on the rear drive-shaft, it was soon discovered to be necessary to bw printed sideways. This is to take advantage of the directional strength of the 3D printed layers so they can tolerate a degree of torque. I made them mistake of printing the shafts vertically, however this drive-train produces incredible amounts of torque such that the plastic drive shaft failed in 10 seconds of run time. Layer adhesion alone is definitely not strong enough to withstand a large torque being applied to a small cross-section.\nI wanted to make my design as simple as reasonably possible. In my previous projects, trying to implement a suspension was fairly complicated and I had limited spare time I had outside of my 9-5. My only goal was to get a suitable functional prototype car.\nI did end up using the lessons from my previous failed car designs to incorporate into this one. I had a 2 year hiatus from designing anything mechanical, so I surprised even myself when I managed to build out a working 3D printed gearbox.\nIt\u0026rsquo;s not perfect by any stretch of the imagination; after the assembly I still had to take a dremel to various parts to shave off some millimeters in order to free up mechanisms and part fits. Even after checking for collisions in Solidworks, the clearances and fits may not be what you expect in real life.\nIf I design more cars in the future, I will try make more thoughtful and elegant designs with improved tolerancing. Even after 4 years of 3D printing, its not straightforward to nail down the tolerancing. To be honest, I think it is extremely reliant on trial an error to nail down correctly, but at least its at the point of acceptability for this design.\nGeneral Car Stats Dimensions: Approx. 300mm x 230mm Weight: Approx 1.2kg (including electronics) Motor: 550/35T Battery: 11.1V 3S LiPo Servo: MG995-R Plastic Used: PLA Design Photos Here are some renders made in Solidworks:\nFront View Side View Top View Rear View Front Wheel Hub - Close Up Gearbox - Close Up Steering Articulation Gifs ","permalink":"https://filpill.github.io/projects/2023-06-12-3d-printed-car/","tags":["‚öôÔ∏è Engineering"],"title":"3D Printed RC Road Vehicle"},{"categories":null,"contents":"Summary This article will briefly outline the basic arrangement that can be used as guidance for your DIY RC electronics.\nParts List Any of these specific parts that are mentioned here can be interchanged for your specific needs. The list of parts outlined is the minimum number of components to have a functioning motor and servo.\nComponent Part No. Radio Controller/Transmitter FS-GT5 Radio Reciever FS-BS6 Electronic Speed Controller (ESC) KYRC X60 - Brush Battery Eliminator Circuit (BEC) KYRC X60 - Brush Lipo Voltage Indicator BX100 Brushed/Brushless Motor 550/35T Brushed Motor Servo MG-996R LiPo Battery 11.1V 3S HRB Power Battery Balance Charger/Discharger HTRC T150 Type of Electrical Connectors These are the electrical connectors used in these components.\nComponent Connector(s) ESC Traxxas, Deans, Motor Pin, Futaba Battery Traxxas, Deans Motor Motor Pin Servo Futaba Voltage Alarm JST XH Balance Charger JST XH Typically the battery connectors can come in a number of varieties which can make the matter confusing. However, you always have the option of hooking up and adapter.\nIn my case, I had to use as Deans (male) to Traxxas (female) adapter to complete the circuit.\nBlock Diagram - Connections Here is the component block diagram:\nmindmap root((ESC)) Motor BEC Reciever Servo Battery Voltage Indicator The ESC as the hub of the system in which everything connects to. The ESC on my system also happens perform the functionality of the BEC and can be considered as one unit.\nRadio My radio Rx/Tx system came as a package, and were already \u0026ldquo;bound\u0026rdquo; together already. As long as I could supply a current to the receiver, it automatically connect to the transmitter.\nIf you do not have binding (or need to use a new receiver), there are usually specific procedures to follow for your given controller to bind them together (in the manual for the controller).\nThere are no complicated procedures outside of this, you just need to follow the block diagram and make sure the components are rated for the supplied voltage/current.\nThe radio transmitter I used has 6 channels.\nFor reference my servo was plugged into Channel 1 (CH1), my ESC/Motor was plugged into Channel 2 (CH2).\nThere are still 4 empty channels open for other electronics such as additional servos can supplement your RC controls.\nLiPo Battery Maintenance Safety Precautions should be taken when using LiPo batteries, they can be very dangerous if mishandled.\nStorage If you are not using the LiPo\u0026rsquo;s when they are in a \u0026ldquo;charged\u0026rdquo; state, then you should consider discharging them to a \u0026ldquo;storage voltage\u0026rdquo;. A safe storage voltage per cell is somewhere between 3.6V and 3.8V.\nCharging The maximum allowable voltage of a LiPo cell is 4.2V per cell. Do not overcharge the battery as you may cause damage to the battery and reduce the lifespan; it may also become a fire hazard if consistently overcharged.\nIn terms of charging rates, I am charging my batteries at a rate of 1C. For a 2200mAH battery this equates to 2.2A.\nDischarging The inverse is true about discharging the battery too much. Do not let the LiPo voltage fall below 3.2V per cell as this will also cause internal damage to the battery which is irreversible. To mitigate this risk, you can use a voltage alarm on your circuit to warn you when the voltage is falling below a desired threshold.\nSigns to Discontinue Battery Usage A well maintained LiPo battery should be able to safely achieve at least 300 full charge/discharge cycles if obeying the precautions above.\nThere are a list of signals decide whether or not to discontinue use of any given LiPo:\nIf you see ballooning occur in the battery. Battery not holding a charge and voltage drop-off. Battery getting warm when charging at a rate of 1C. Physical dents or damage to the battery increasing the internal resistance. Gifs Following the block diagram, you should be able to achieve these results.\n","permalink":"https://filpill.github.io/projects/2023-06-07-radio-control-setup/","tags":["‚öôÔ∏è Engineering"],"title":"How to Set Up Your Radio Controlled System"},{"categories":null,"contents":"Introduction This guide details the installation procedure of Arch onto a Virtual Machine (VM). Network configuration is not in the scope of this guide if you are installing on bare metal.\nThe motivation behind this web page is to keep some guidance material for myself and also for anybody new looking to install Arch from scratch.\nThe virtual machine is made with Microsoft\u0026rsquo;s Hyper-V program. The steps may differ slightly if you choose to use a another hypervisor such as VirtualBox.\nVideo Tutorial This video tutorial will closely follow the installation process described in this article:\nArch Linux ISO Download Link You can find the download link to the Arch Linux ISO here: https://archlinux.org/download/\nThe recommended way to pull down the ISO file is by torrenting it from the referenced magnet link.\nWhen the download is complete, you can configure your virtual machine.\nCreating a Hyper-V Virtual Machine 1. Create New Virtual Machine The first step is to open up Hyper-V and create a new virtual machine 2. Select VM Generation You will be asked which generation of VM to create. I would reccommend to select \u0026ldquo;Generation 2\u0026rdquo;. It supports UEFI firmware and it should be better from a cyber-security standpoint. 3. Allocate Memory You can specify how much memory to allocate to the VM. I decided to allocate 4000MB in my use case. 4. Configure Network Adapter Hyper-V has a built in network adapter that works out of the box, so you\u0026rsquo;d want to select \u0026ldquo;Default Switch\u0026rdquo; to have it enabled. 5. Create Virtual Hard Disk For the VM you should create a virtual hard disk to store the data. I decided to allocate 30GB of storage space. 6. Select .iso Image You need to select the arch linux ISO file to boot from and then you can finalise the VM creation. 7. Go to Settings We are not done yet, we still need to configure 2 more things in the settings panel of this VM we just created. 8. Disable Secure Boot We need to disable Secure Boot, if you do not do this step, it will interfere with your ability to boot the ISO image. 9. Change Boot Priority Next we need to bump the Hard Drive to the first priority of boot loading. Its very important that you do this step before you start installing linux otherwise the Grub will fail down the line because it will be trying to boot from the DVD Drive. You will not be able to retroactively change the boot order at a later stage. After applying the changes:\n10. Fixing Hyper-V Resolution (Powershell) Oddly enough we do not have GUI setting for adjusting the resolution of the VM which is bizzare for a windows tool. Therefore we will need to use Powershell (with Admin permissions) to fix this.\nIn powershell you can run the following command to set the maximum resolution of the VM (Please replace the respective VMName in quotations with the VM you just created.):\nSet-VMVideo -VMName \u0026#39;ArchVM\u0026#39; -HorizontalResolution 1920 -VerticalResolution 1080 -ResolutionType Single 11. Start the VM Connect to the VM: Start the VM: Installing Arch Linux From this screen select the highlighted option to load the ISO image The install process starts here: 1. Verify Internet Connection Before going any further, you should ping a website to test your internet connection:\nping archlinux.org The Hyper-V default network switch should automatically provide your VM with the internet connection. If not, you will probably need to configure Hyper-V to alleviate networking issues.\n2. Synchronise System Clock We also need to synchronise the systems clock with the network time:\ntimedatectl set-ntp true 3. Drive Partitioning Now we need to start partitioning our drives and creating our file system.\nThe lsblk command indicates which drives and partitions are on your system. And we can use cfdisk to partition the drives. Type the command to start partitioning your virtualised disk:\ncfdisk /dev/sda At this juncture we need to assign the disk label type and it comes down to two options mainly:\nIf the hardware is new and the drive \u0026gt;= 2TB, use gpt. Otherwise use dos. For our scenario we are using dos\nDecide on a partition scheme. I will keep it simple and reduce it to a boot and root partition:\n/dev/sda1 (boot partition) only needs about 128MB of space and needs to be bootable. You can press b to enable the boot flag after making the partition. /dev/sda2 (root partition) can be assigned the remaining space of the disk. Write the changes and quit out of cfdisk. If you lsblk now, you will now see that 2 partitions have been made which are called sda1 and sda2.\n4. Creating File Systems We also need to make the file systems for each partition:\nsda1 (boot) is FAT32 file system. sda2 (root) is ext4 file system. Run these commands to create the file systems respectively:\nmkfs.fat -F 32 /dev/sda1 mkfs.ext4 /dev/sda2 5. Mounting the File Systems Now the mounting procedure MUST go in a specific order:\nsda2 (root) is mounted first to /mnt sda1 (boot) is mounted last to /mnt/boot/efi mount /dev/sda2 /mnt Warning: If you mount in reverse order, the install will fail!\nFor the boot partition, you will need to make a directory with this path /mnt/boot/efi:\ncd /mnt mkdir boot cd /mnt/boot mkdir efi Since we have a UEFI system config, we need this extra efi directory (normally we do not). Now we can mount the filesystem for boot:\nmount /dev/sda1 /mnt/boot/efi 6. Installing Essential Packages At this stage, you are ready install the base tools for the system in addition to the Linux kernel.\nUse the pacstrap command to install the following tools to /mnt:\npacstrap /mnt base base-devel linux linux-firmware vim 7. Generate fstab Configuration Run the following command to generate your fstab configuration:\ngenfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab genfstab -U /mnt 8. Chroot Change root into new system by running this command:\narch-chroot /mnt /bin/bash 9. Install Key Packages Pacman is our package manager and we can use it to install some key packages:\nnetworkmanager - Detection and configuration of systems to connect to networks grub - Bootloader for the computer efibootmgr - Package dependency for grub\u0026rsquo;s installation command pacman -S networkmanager grub efibootmgr 10. Auto-enable Internet on Startup We can use systemctl to enable network manager as a service which starts automatically when you boot the computer (which is very useful to have).\nsystemctl enable NetworkManager 11. Install and Configure Grub Next we run the grub-install command on /dev/sda; it should run with no errors:\ngrub-install /dev/sda We need to make a config file for grub, with the following command:\ngrub-mkconfig -o /boot/grub/grub.cfg We need to edit this grub file:\nvim /etc/default/grub Uncomment the line \u0026ldquo;GRUB_DISABLE_OS_PROBER=false\u0026rdquo; and then save the file.\nYou may also want to edit the following grub variables if you wish to change the resolution of grub:\nGRUB_CMD_LINUX_DEFAULT GRUB_GFXMODE GRUB_CMD_LINUX_DEFAULT=\u0026#34;quiet splash video=hyperv_fb:1920x1080\u0026#34; GRUB_GFXMODE=1920x1080 Note: The video parameter may be different if you are doing this bare metal or are using another VM software.\nSave and exit from the grub file.\nNow you will need to enact those changes onto the grub.cfg file which is read at boot time. So you will need to run the following command:\ngrub-mkconfig -o /boot/grub/grub.cfg After rebooting your Linux machine, your terminal should automatically scale to the 1080p resolution without having to make changes using commands like xrandr -s 1920x1080 inside the display server.\n12. Set Root Password Run the passwd command to set a root password for the machine 13. Localization Edit /etc/locale.gen to change the language settings of the machine.\nvim /etc/locale.gen Uncomment the lines that apply to your language of choice. Run locale-gen command to generate the locales you have selected:\nlocale-gen Edit /etc/locale.conf and set the language being used:\nvim /etc/locale.conf Enter the LANG variable into the file with the relevant language setting:\nLANG=en_GB.UTF-8 14. Hostname Edit /etc/hostname:\nvim /etc/hostname The word you type in here will be the name of your computer. I decided to call mine arch. But it can be anything you want. Save and close the file.\n15. Timezone You can set the time zone with the following command:\nln -sf /usr/share/zoneinfo/Greenwich /etc/localtime Greenwich can be substituted for any region within that zoneinfo directory.\n15. Rebooting We are ready to reboot the system. First we need to exit the root environment and to unmount all the partitions. Then we can reboot.\nexit umount -R /mnt reboot If successful you should be greeted with this screen. You can select Arch Linux to boot into the system. And you should see a tty appear on your screen.\nWe do not have any users set up on the system (yet).\nSimply login as the root user. The root password is what you set earlier in the setup process.\nThat\u0026rsquo;s about it, you now have a base install of Arch Linux.\nFrom this point you are free to customise your desktop to your liking.\n","permalink":"https://filpill.github.io/projects/2023-05-18-arch-install-guide/","tags":["üíª Systems"],"title":"Arch Linux Installation Guide"},{"categories":null,"contents":"Summary I\u0026rsquo;ve played a decent number of chess games on the chess.com platform since 2019 and all my chess data can be explored using the API endpoints documented inside the website.\nI was interested in monitoring my rating change over the cumulative number of games played.\nLink to the project here: Chess.com - Python Analysis\nAnalytics Process Flow graph TD; subgraph Process Initiation 0([Python Notebook Executed])--\u003eA[HTTPS Request Chess Username Stats] end subgraph Data Extraction A--\u003eB[Requesting List of Month Endpoints] B--\u003eC[Request List Games For Each Month Endpoint] C--\u003eD[Unpack all Games into Dataframe] end subgraph Data Cleaning D--\u003eE[Filtering to Blitz Chess Games] E--\u003eF[Cleaning pgn Chess data\nand Merging Additional Attributes] F--\u003eG[Extracting Player Ratings\nand Game Results] G--\u003eH[Curating List of Columns] end subgraph Data Aggregation H--\u003eJ[Top Openings] H--\u003eK[Time of Day] H--\u003eL[Monthly Volume] H--\u003eM[Game Result] end subgraph Visualisation J--\u003eN[Graphing Openings] K--\u003eO[Graphing Time of Day] L--\u003eP[Graphing Monthly Volume] M--\u003eQ[Graphing Game Result] end subgraph XlsxDashboard N--\u003eR[Insert and Format Tabular Data] O--\u003eR P--\u003eR Q--\u003eR R--\u003eS[Insert Matplotlib Charts] S--\u003eT([Close Xlsxwriter Workbook Object]) end Data Visualisations The following images are served directly from my github repository:\n","permalink":"https://filpill.github.io/projects/2023-04-10-chess-data/","tags":["üìä Data"],"title":"Chess.com API - Data Visualisation"},{"categories":null,"contents":"Summary I\u0026rsquo;ve been very involved in practising my programming skills in python and SQL lately on codewars.\nI figured it would be a good idea to visualise my codewars progression.\nI\u0026rsquo;m utilising the codewars public API to pull out my information. The user data is processed in Python with the output being consolidated into a Excel dashboard using Xlsxwriter.\nLink to github project: https://github.com/Filpill/codewars-stats\nAnalytics Process Flow graph TD; subgraph Process Initiation 0([Python Notebook Executed])--\u003eA[HTTPS Request Codewars Username Stats] end subgraph Return JSON A--\u003eC[Username Profile Data] A--\u003eD[List of Compeleted Kata by User] A--\u003eE[Individual Challenge Details] end subgraph Merge Data D--\u003eF[Merge Data via the Kata ID] E--\u003eF F--\u003eG[Re-name Columns to Better Format] end subgraph Count Aggregates G--\u003eH[Categories] G--\u003eI[Monthly Completions] G--\u003eJ[Kata Rank] G--\u003eK[Languages] end subgraph Visualisation H--\u003eL[Graphing Cats.] I--\u003eM[Graphing Comp.] J--\u003eN[Graphing Rank.] K--\u003eO[Graphing Lang.] end C--\u003eP subgraph XlsxDashboard L--\u003eP[Insert and Format Tabular Data] M--\u003eP N--\u003eP O--\u003eP P--\u003eQ[Insert Matplotlib Charts] Q--\u003eR([Close Xlsxwriter Workbook Object]) end Data Visualisations These images are served directly from my github repository:\n","permalink":"https://filpill.github.io/projects/2022-11-09-codewars-data/","tags":["üìä Data"],"title":"Codewars API - User Statistics"},{"categories":null,"contents":"Summary I\u0026rsquo;ve been in the process of developing numerous Tableau dashboards across different areas of the airline business to share safety data across the organisation.\nHowever, there is no easy way to download all the various charts from dashboard views you have produced to recycle for the purpose of a PowerPoint presentation.\nIdeally we want to avoid duplicating the data analytics process in other systems such as Python as we don\u0026rsquo;t want to incur unnecessary work to present the same information.\nThe Tableau Client Server API helps solve this issue by enabling us to programmatically extract dashboard images in an efficient manner.\nProcess Overview iPython/Jupyter Notebook Connecting to the Tableau Client Server API with personal access token. Collating all Projects and Views into table and utilising view id\u0026rsquo;s to extract dashboard images in conjunction with Pandas. Dashboard images cropped down to chart dimensions with a Python Image Library prepare image for presentation use. Link to the project here: Tableau Dashboard Image Extraction Notebook\nTableau Client Server API Workflow graph TD; subgraph Process Initiation 0([Python Notebook Executed])--\u003eA[Personal Access Tokens: Authenticating Valid API Connection\nwith Tableau Server] end subgraph Get Requests: Server Object ID's A--\u003eC[Gathering Project ID's] A--\u003eD[Gathering Workbook ID's] A--\u003eE[Gathering View ID's] end subgraph Combine \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Data C--\u003eF[Merging ID Info into Single Pandas DataFrame] D--\u003eF E--\u003eF F--\u003eG[Filtering DataFrame to Selected Project ID] G--\u003eH[Collecting List of Server View Objects] end subgraph Process \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Images H--\u003eI[Saving all the Images of View Objects] I--\u003eJ[Python Image Library\nCrop Images to Correct Dimensions] J--\u003eK([Save Cropped Dashboard Images]) end Conclusion As you can see, the process for extracting the Tableau image data from dashboards is fairly straightforward.\nWith the View ID, you are able to extract the full dashboard image. The only drawback is that there is no way to pull out the individual charts from the dashboard view via this API. Therefore you need to specify all the pixel dimensions to crop the images into their respective charts.\nThe only alternative is to implement a web-scraping technique on Tableau Server and figure out a way to pull down the chart data. It would probably require something like Selenium because of the dynamic nature of the website.\nRegardless, with the Tableau Client Server API you will still benefit from the dashboard image extraction in addition to allowing you to repurpose your existing dashboards for presentation purposes.\n","permalink":"https://filpill.github.io/projects/2022-09-16-tableau-client-server/","tags":["üìä Data"],"title":"Tableau Client Server API: Extracting Dashboard Images"},{"categories":null,"contents":"Summary There are endless opportunities to streamline processes and automate our data production tasks.\nThis article serves as an example for the wide ranging applications I\u0026rsquo;ve been using Python to automate both simple and complex tasks with high efficiency.\nData Pipelines These are some of the goals we are trying to achieve:\nTo build complete end-to-end data solutions from the data source to user. We want to minimise manual task interventions during the data processing. To build tools which are adaptable to the needs of the business. This is a high level view of a data analytics pipeline I\u0026rsquo;ve built in the past:\ngraph TD; subgraph Process Initiation 0A([Data Analyst Initiates Process])--\u003eA 0A--\u003eB end subgraph Data Pipelines A[Shell Scripts]-- exec. via cli tools --\u003eC((Python Scripts)) B[Task Scheduler]-- auto exec. --\u003eC((Python Scripts)) D[(SQL Server)]-- data retrieval --\u003eC C-- pyodbc --\u003eD C--\u003e E(Matplotlib charts) C--\u003e F(Pandas tables) E-- saved as .png --\u003e G(Saved In Network Drive) F-- saved as .csv --\u003eG subgraph Create Data Product G-- xlsxwriter --\u003eH(Excel\nDashboards) G-- python-pptx --\u003eI(Powerpoint\nPresentations) end end subgraph Process Termination H--\u003eJ(Emails sent via SMTP) I--\u003eJ J-- smtplib --\u003eK([End User Recieves Data Product]) end Note: Python scripts are chained together to connect the processes together and handle various tasks along the pipeline\nThis is the result of incremental upgrades made gradually over time to optimise my workflow. I\u0026rsquo;ve saved much time inter-weaving these python libraries together.\nAs long as the tools are well built, you can bridge them across to new processes easily and maintain them with minimal effort. Not to mention you will be saving hundreds (and maybe thousands of) hours as you execute these processes.\nThe idea is not to replicate this system one-to-one, but to demonstrate the possibilities for connecting automation tools together.\nAnd perhaps we can learn to make more elaborate systems\u0026hellip;\nPyODBC - A SQL Interface For Python The most important setup of the automation process is the ingestion of data into the Python environment.\nFortunately for myself, our organisation hosts several Microsoft SQL Servers which form part of our data warehouse.\nPyODBC is a python module which can authenticate and connect directly to your desired SQL server. The great thing about it is that you can wrap an entire query written in SQL with a triple quote string directly within your python script. The module will be able to send the query out using the relevant ODBC driver on your machine.\nThe pyodbc set-up can be generalised because you only need two inputs: the server name and the sql query. Therefore I have developed a python utility written which can be imported into any data processing script as a module.\nXlsxWriter - Excel Report Generator For Python The proliferation of Excel as a standard piece of software has enabled much easier sharing and communication of data. There is no office computer in the world that is not supplied with Excel.\nBI tools are also used to communicate and share data across the business. However, this depends on how much the organisation can stretch the budget to license the majority of individuals.\nWorking inside the Excel ecosystem can help save on the extra overhead. However, we don\u0026rsquo;t want to be doing any analysis in Excel.\nWhy?\u0026hellip;\nBecause it\u0026rsquo;s extremely computationally expensive to handle large data sets. And its next to useless when stringing together a series of complex formulas to analyse data.\nHow can we solve this issue? Simple \u0026ndash; Xlsxwriter.\nXlsxwriter is a python module which allows you to create fresh Excel reports from scratch with analysis conducted in Python. You can turn all your pandas dataframes into Excel tables. Additionally, matplotlib charts can be inserted into the workbook to act as the visualisation component.\nWith a bit of effort, you can easily turn the analysis into a dashboard that mimics the appearance of an Excel spreadsheet. I frequently use this tool to get dashboards into production quickly with python.\nNote: XlsxWriter as the name suggests \u0026ldquo;writes Excel Workbooks\u0026rdquo;. To be clear, this is not an API that can directly interact with objects of pre-existing workbooks. Everytime you execute a script with xlsxwriter tools involved, a completely new file will be produced and overwrite the previous version.\nPython-Pptx - PowerPoint Presentation Generator For Python How many hours do you waste making PowerPoint\u0026rsquo;s? I\u0026rsquo;ve spent countless hours adjusting trivial features such as formatting or chart positioning.\nBut what if the presentation content and design can be prepared automatically?\nI generate tens of presentations per month to cater to multiple stakeholders. Given that I have fairly standardised slide content, this makes presentations a high value automation target for myself.\nI\u0026rsquo;ve experimented with markdown presentation tools. However, I run into a lot of issues where I do not have full positional control of the objects.\nStakeholders tend to ask for extra features or to move shapes around the slide. However, this is happens to be the power of python-pptx library.\nPython-pptx is much more powerful in controlling the PowerPoint design in comparison to letting markdown dictate all the default positions of your PowerPoint shapes.\nThe control of the objects is very granular its therefore very important to set up functions to standardise some layouts or shapes you want to insert. This speeds up the presentation building process as usually slides tend to borrow a lot of the same code.\nThe PowerPoint functions I write are imported from a separate .py file. A global function file helps us share the functions to any python script involving presentation automation.\nNote: Python-pptx works in similar fashion to xlsxwriter. Every time the script is run, a brand new presentation file will be built from scratch.\nSmtplib - Email Automation with Python Routine emails sent on a recurring basis can be automated using the python library smtplib.\nWe can borrow the SMTP protocol to send our emails and we can avoid our GUI email environment entirely. Python acts as the interface and can send the email directly.\nThe script can be designed to have a message template built into itself. Additionally, we can make a list of tuples containing the addresses and attachments you want to send to the respective parties.\nBy looping over these templates and lists, we can entirely bypass the requirement of building all the components for all the emails.\nTens of emails can be sent out in a matter of seconds.\nConclusion Considering the tasks we do daily,weekly and monthly; we can tie our procedures together with an elaborate series of Python tools to save hundreds, if not thousands of hours.\nFor most organisations, the cost benefit ratio is extremely favourable if automation systems are applied (and maintained) correctly.\nThe value of your time increases exponentially as you are able to prioritise resources in creating higher quality work.\n","permalink":"https://filpill.github.io/projects/2022-07-23-python-pipelines/","tags":["üìä Data"],"title":"Building Data Pipelines With Python: Systems Perspective"},{"categories":null,"contents":"Summary This article documents the most common ways I interact with git and in maintaining my programs.\ngraph LR; subgraph High-Level Process direction LR 0[Installation]--\u003e1[SSH] 1 --\u003e 2[Git Configuration] 2 --\u003e 3[New Repository] 3 --\u003e 4[Commit Changes] end Why should I use Git with the Command Line? Git was designed as a command-line tool, therefore it\u0026rsquo;s considered the de-facto way to interact with Git\u0026rsquo;s tools.\nAny other versions of Git are just wrappers for what Git does on the command-line. GUI type Software built \u0026ldquo;around Git\u0026rdquo; has tendencies to introduce bugs which are difficult to resolve.\nYou are abstracted away from what Git is actually doing, so it\u0026rsquo;s not possible to tell if the error was with action performed in Git or if the software was somehow glitched.\nOther tools may also rebrand some git commands to other names which can present communication issues if people are using different flavours of Git.\nBut its worth building some knowledge around the concept of how to use the Shell. Bash (shell) scripting is worth its weight it gold if leveraged correctly. It\u0026rsquo;s great for task automation and shell is a transferable skill to other command-line type programs, e.g. bq command-line tool for GCP.\nConnections and Authentication There are 2 main ways to connect to a git repository. We can either use the HTTPS or SSH protocols respectively.\nSSH has the advantage of having password-less interactions with git which I strongly recommended when pushing code. Its trivial to set-up an SSH key-pair; it saves time and its very secure. (Assuming you take the right precautions to protect your ssh-keys!)\nMy deep personal recommendation is to use the SSH Method and avoid using the access tokens for authentications.\nSomething worth noting is that Git will search the path for the ssh-key file ~/.ssh/id_rsa by default. Make you name your file id_rsa otherwise it will not work.\nCreating SSH Keys Using ssh-keygen Go to your ~/.ssh directory which is storing ssh-keys Type this command to generate an SSH Keypair: ssh-keygen -t rsa -b 4096 -C emailname@domain.com Name your SSH keys (preferably \u0026ldquo;id_rsa\u0026rdquo; for the default filename), and skip password prompts \u0026ldquo;Cat\u0026rdquo; out the public key (suffixed .pub) that was generated and paste into the Github settings for SSH Keys For security purposes, permissions need to be changed for the keys, you can change them using the below command prevent external read/write access: chmod 600 ~/.ssh/id_rsa chmod 600 ~/.ssh/id_rsa.pub Handling Multiple SSH Keys Using Keychain If you happen to decide to put your keys outside the default path (~/.ssh/id_rsa), git won\u0026rsquo;t search for them.\nI\u0026rsquo;m using a program called Keychain which can help manage your SSH Keys more easily. Its driving the underlying ssh-add and ssh-agent commands.\nI\u0026rsquo;ve added this line (shown below) to my .bashrc in order to add the relevant SSH key to the keychain.\nWhen I spawn a terminal, it will activate this command automatically and start a new process.\neval $(keychain --eval ~/.ssh/keys/arch_vm) #2\u0026gt;/dev/null In order to do the same process manually, these are the following commands:\neval `ssh-agent` ssh-add ~/.ssh/keys/arch_vm Git Basic Setup Setting Up Git Config Inside your home directory you should have a file called .gitconfig which should be empty on a fresh install.\nYou can define the username and email of your github account in here with the following commands:\ngit config --global user.name \u0026#34;github_user_name\u0026#34; git config --global user.email \u0026#34;emailname@domain.com\u0026#34; Create Git Repository This is the overall process of creating a repo on the CLI: graph LR; subgraph Creating Git Repository 0[Create README with Repo Name]--\u003e2[Initialise Git] 2 --\u003e 3[Stage Changes] 3 --\u003e 4[Link Local Repo to Remote Repo] 4 --\u003e 5[Commit Changes] end Create a README and initialise git with these commands: echo \u0026#34;# repository_name\u0026#34; \u0026gt;\u0026gt; README.md git init By default the branch is called \u0026ldquo;master\u0026rdquo;, however this can be renamed to anything you desire.\nLets rename \u0026ldquo;master\u0026rdquo; to \u0026ldquo;main\u0026rdquo; for example using this command:\ngit branch -m main Staging is the process of preparing the changed files for the next commit.\nYou can stage the files with add; the full stop will include all files that have changed. Or you can explicitly write which files you want to add to the commit.\nThe commit command will create the commit, (the -m flag allows you to add a commit message inline):\ngit add. git commit -m \u0026#34;first commit msg\u0026#34; To connect the remote repository to the Github server, you can write this command:\ngit remote add origin git@github.com:github.com:github_username/repo_name.git To push your first commit into Github, write this command (the -u flag sets a tracking reference upstream for the git pull command):\ngit push -u origin main Cloning a Github Repo Go to Github and copy ssh address of repository Go to the directory where you want to clone the repository into and type: git clone git@github.com:github_username/repository_to_be_cloned.git Commit Changes to Github Repository After using Git for much time on personal projects it\u0026rsquo;s become very ritualistic for me to fall into this pattern:\ngraph LR; subgraph Basic Workflow direction LR 0[Pull]--\u003e1[Add] 1 --\u003e 2[Commit] 2 --\u003e 3[Push] end git pull origin main git add . git commit -m \u0026#34;add some comments here\u0026#34; git push origin main Pull the most recent changes from repo Make amendments to your work and save it locally Queue up changes to upload Commit the changes Push the changes onto the checked out branch Working with branches In most scenarios, you will not be working directly with the main/master branch. Working directly on main can be fraught with peril as you are constantly pushing every change into a production environment.\nThe purpose of branching off the main branch is to create an isolated development environment where you can safely change and push code to that branch without changing the production state. Especially if you haven\u0026rsquo;t fully tested or validated your code/commits.\nYou can can call branches whatever you want, but lets say you want to create development branch called \u0026ldquo;dev\u0026rdquo; from the branch you are currently on, you can do so with the git checkout command:\ngit checkout -b dev Now you have a dev branch. The -b flag essentially tells git that you want to create a new branch.\nAny commits you make on the dev branch will have to be pushed up to the dev server branch. This can be done like so:\ngit add . git commit -m git push origin dev When you think your development is complete and you commits are ready to be put into production, you must perform a git merge operation.\nIn order for this to occur, you must switch back to the main branch of your repository before applying the merge:\ngit checkout main git merge dev And now all your \u0026ldquo;development commits\u0026rdquo; should be merged into the main branch.\nIf you want to see what other branches currently exist in your repository, you can simply type:\ngit branch -a Conclusion This guide is designed to get you up and running with git on the CLI and understand the fundamentals of interacting with git.\nWith respect to the git\u0026rsquo;s numerous other features, it\u0026rsquo;s recommended to read git\u0026rsquo;s documentation to understand how to use the tool in more depth.\n","permalink":"https://filpill.github.io/projects/2022-07-17-github-cli/","tags":["üíª Systems"],"title":"Github - Command Line Interface Procedures"},{"categories":null,"contents":"Summary I wanted to showcase my projects without devoting a large amount of time to maintaining HTML code and needed a framework to make it easy to share my projects.\nHugo is a static site generator written in Go and compiles your documents written in markdown into cleanly formatted HTML and CSS files.\nThe first iteration of my website was manually written in HTML/CSS. It was a little bit ugly compared to the second iteration which is using hugo framework. Additionally, using someone else\u0026rsquo;s theme takes a lot of the legwork out of the page design.\nThe compiled HTML files are hosted on my github pages for reference.\nLink to the Hugo Markdown Files\nLink to Static HTML Hotsted on Github Pages\nProcess Here is some general guidance as to how this website was developed from a Linux OS perspective. The guidance should be adapted according to the OS being used:\nGithub Repositories To build and deploy the website, we require two separate repositories:\nA production repo blog-web: with markup documents and web config. A deployment repo Filpill.github.io: with compiled static HTML files. Content is added to the markdown documents in the production repo. This is later compiled into static HTML.\nThe static files reside in the public folder. They are eventually pushed into the deployment repo.\nThe deployment repository exists so we can allow github to display the HTML files.\nInstalling Hugo on Arch Linux I\u0026rsquo;m working from an Arch based Linux OS at the time of development. Installing the hugo on arch can be done with the following pacman command: sudo pacman -S hugo Installing Hugo on Windows Update 17/02/23: I\u0026rsquo;ve added instructions for a Windows Installation for convenience. (I spend too much time switching between operating systems for certain tasks.)\nThere are two pre-requisites for setting up hugo - you must install:\nGit Bash GoLang We must make a set of directories in which we can install hugo:\ncd ~/ mkdir go cd go mkdir bin mkdir pkg mkdir src cd src mkdir github.com cd github.com mkdir gohugoio At this point we are able to clone the hugo repository and perform the install:\ncd gohugoio git clone https://github.com/gohugoio/hugo.git cd hugo go install You can confirm hugo\u0026rsquo;s been installed by checking this path:\n$GOPATH/bin/hugo Creating New Website Initialise Website Folders To initialise a new website type the following command into your shell: hugo new site name_of_your_website This will create all the folder templates you need to get started. Website Theme There are a couple of approaches to this, you can either design your own theme or you can choose one that is publicly shared. I chose to use a repository containing a theme called PaperMod which is a minimalistic theme which fits my requirements. cd into your themes directory and clone the repository at that location. I\u0026rsquo;m choosing to use the SSH method to clone into it, however you can also use HTTPS method if you prefer: git clone git@github.com:adityatelange/hugo-PaperMod.git Configuration File The configuration document is in the root directory and is called config.toml This file configures various aspects of your website. How to make a webpage Hugo\u0026rsquo;s webpages reside in the content folder of your website.\ncd to the root of your website To make a new page type: (replace the page_folder and page_name with names of your choice) hugo new page_folder/page_name.md The name of the page and directory automatically form links for your website A new markdown file with a default template will appear for your page. Using markdown, you can populate the file with your webpage content. Images Images are stored in one of two directories. Either the static or assets folder:\nThe static folder exists if you wish to directly use your image in their original state via markdown. You may wish to optimise the image by scaling down the resolution or compressing the image, and this will only be the case if you call the image in the assets folder. You can make some HTML shortcodes to standardise the optimisation you want to apply to the image. Technically .gif files are not optimal inclusions into minimal websites due to the large file size. Although I like the visual presentation they provide. Compiling on Local Server Observing the compiled version of the (draft) website is simple, type the following command:\nhugo server -D Adding the -D argument at the end also compiles the documents with the draft status set to true. Omitting the -D at the end of the command will not render documents in draft state. The local website host is at this address: http://localhost:1313/ Adding a Submodule A git submodule is a record within that points to a specific commit in another directory.\nThis is a key component of this workflow as we want to send the contents of our public folder and point it to an external repo where we are hosting.\nIn our case want our deployment repo Filpill.github.io to be a submodule of the projects repo.\nTherefore we want to cd into the root folder of our projects website (blog-web) and type:\ngit submodule add -b main git@github.com:Filpill/Filpill.github.io.git public And this enables us build from blog-web and to push code from the public folder of blog-web straight to Filpill.github.io\nCompiling Static Files into Public Folder In order to compile the files necessary for the deployment repository on git. Type the following command in the root directory of website:\nhugo -t hugo-PaperMod -D The argument after \u0026ldquo;-t\u0026rdquo; is theme which is being used to compile the website together. In my case I\u0026rsquo;m using hugo-PaperMod. The resulting files will compile straight into the public folder. Again similarly to the local host, adding a \u0026ldquo;-D\u0026rdquo; will transform draft pages as well as the finalised pages. You can choose to omit the \u0026ldquo;-D\u0026rdquo; when you are finalising the website. But you must change the draft state in the markdown pages. Deploying pages onto your Github Since all the static files are compiled in public and we have that folder pointing towards the deployment repo. We can just write our git commands to push the changes into deployment.\ncd public/ git add . git commit -m \u0026#34;deploying compiled html\u0026#34; git push origin main At this stage we have website up and running and hosted on github. Adding new content to the website is easily achieved with new markdown files. And changes are pushed with the previous 2 steps. Or we could script this out completely with a shell script.\n","permalink":"https://filpill.github.io/projects/2022-06-03-building-hugo/","tags":["üíª Systems"],"title":"Website Design with Hugo Framework"},{"categories":null,"contents":"Summary Designed a 3D printed lithophane lamp in Soldworks. Main feature: Replaceable lithophanes of chosen standard of aspect ratio/dimension. Features Top/Bottom Panels retained with three M3 screws \u0026ndash; Removing the panels gives access to replacing the printed lithophane. Wiring threads through back of assembly and socket leans on back the back panel. Forward attachment clamps the socket and constrains the forward movement of the light socket with screws. I have two slightly different designs to accommodate either an E14 lamp socket or an E27 lamp socket. This was due to supply issues of light sockets at the time of design which prompted a redesign due to time constraints. Printing and Design Images are preprocessed to correct specs and lithophane stl\u0026rsquo;s are generated using: https://itslitho.com\u0026quot; Lithophane thickness parameters: smallest wall thickness 0.9mm largest wall thickness is 3mm border thickness: 5.25mm Printed on Creality Ender 3 pro: 0.4mm nozzle Total print time is 26 hours (my printer set-up) Animation The version shown below in the pictures is the E14 litho lamp: Max Width = 185mm Height = 130mm ","permalink":"https://filpill.github.io/projects/2021-11-13-lithophane-design/","tags":["‚öôÔ∏è Engineering"],"title":"Lithophane Lamp"},{"categories":null,"contents":"Summary My elected project for my 4th year in University was to design and build a UAV aircraft. The design requirements were driven out from the 2017 BMFA competition (Payload Challenge). The design had to be capable of flying some circuits with varying volumes of water weighing up to 3.5kg. As project engineer, I was responsible for coordinating the technical requirements and outputs between our sub-teams. Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. Aircraft Design Process The aircraft design process used in this project is an adaptation of the design process learnt in university for commercial passenger aircraft design. The design tools and flight mechanic equations developed previously have been repurposed such to fit the requirements for a UAV application.\nTo narrow the focus of our design point, data is collected on other aircraft configurations focused around payload missions. This data collection and market analysis process is a critical element to estimating the conceptual aircraft size and Maximum Take-off Weight (MTOW).\nThe design feasibility relies on estimating a relatively sensible MTOW. It is expected that early MTOW estimates may vary ¬±20% from the final result, however it is sufficient as a starting point.\nThe entire design process is cyclic and interdependent on other design elements, for example: adjustments in aerodynamic configuration may require a re-assessment of the centre of gravity positioning. There are hundreds of other situations were systems are interconnected and require engineering attention.\nBeyond the conceptual design, we are relying on emulating design practices within the aircraft model building community in order to achieve a mechanical design that does not add any unnecessary weight.\nWe have elected to stick with a highly conventional configuration which has been tried and tested. The materials involved are mixture between balsa wood, plywood and carbon fibre. This list of materials provide a good strength to weight ratio for the mission we are designing for.\nAreas of the aircraft which experience high loading (e.g. landing gear, wing root etc.) have been reinforced with plywood and less stressed areas are built with the less dense materials such as balsa to maintain a low overall empty weight.\nConceptual Design Phase Detailed Design Phase Aircraft Market Analysis Aircraft CAD Concept Design Derive Flight Equations Preliminary CAD Design Select Appropriate Design Point Solidworks Final CAD Estimate MTOW and OWE Early Prototyping and Testing Decide on Aircraft Configuration Design Revisions and Improvement Aircraft Centre of Gravity Estimation Final Manufacturing Output Aerofoil Comparison and Selection Electronic Systems Integration Flight Dynamics Analysis Systems Validation and Testing Material Selection Wind Tunnel Testing Preliminary Structural Analysis Flight Test Solidworks Design Exploded View Standard water bottle storing main payload in fuselage. Supplemented with custom polypropylene wing tanks. Drawing of UAV Assembly (Final Iteration) Conventional aircraft structure with carbon fiber boom connecting tailplane to the fuselage. Manufacturing Early Construction of Aircraft Constructed with sheets of laser cut balsa and plywood jigsawed together into an assembly. Finalised Construction of Aircraft This is the final assembly of the aircraft after gluing/ironing on the skin. Testing Structural Validation Test - Wing Loading Emulated elliptical wind loading distribution on test spar element Spar failure at 8kg of wing loading. Simulating Maximum Wing Loading on Spar Flight Control Testing Installing servos for ailerons and flaps; testing the control mechanisms. Flight test to validate aircraft design and measure flight performance. Flaps and Ailerons Flight Test Pitch Control ","permalink":"https://filpill.github.io/projects/2018-06-17-uav-aircraft-design/","tags":["‚öôÔ∏è Engineering"],"title":"UAV Aircraft Design"},{"categories":null,"contents":"Summary My 3rd year project in University was to design a Forward Facing Step and perform wind tunnel experiments to analyse flow characteristics on that geometry using PIV.\nParticle Image Velocimetry is a practical tool for Aerodynamics analyse flow fields without intrusions in the wind tunnel environment such as sensors or pitot probes.\nIn this experiment the height of the boundary layer is similarly proportioned to the height of the step. In results shown below you will notice two distinct separation regions on the aerofoil. One at the bottom of the step due to the sudden adverse pressure gradient. And another at the top of the step as the flow is unable to sharply turn the corner.\nPrevious Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. Link to PIV Python Code For Results Post Processing\nExperimental Procedure Wind Tunnel Experiment The experimental procedure involves diffusing small oil droplets into the freestream flow and having a sheet of laser light illuminating a cross-section of the airflow. Hundreds of images are captured in the process a high speed camera. Due to the limited field of vision, the results from the top and bottom of the step were recorded separately. Image Post-Processing Images need some further preparation to enable reliable PIV results. Usually this involves enhancing contrast of the images to make the particles visible. Additionally any background noise picked up by the camera needs to be subtracted from the image as we want to isolate the airflow as much as possible. OpenPIV and Python Post-Process Scripts Particle displacements on the images can be determined by performing a cross-correlation on the series of image pairs. In my case, I used OpenPIV software written in Python to extract all the velocity fields at all the times steps recorded. Additionally, I\u0026rsquo;ve written some post-processing scripts for the velocity data text files in Python to animate experimental results. (I used MATLAB equivalents for my dissertation at the time of the project). Experimental Arrangement Illustration of camera and laser positioning to capture wind tunnel data. Raw wind tunnel image with imageJ enhancements - Capturing illuminated particles moving over step within laser plane. Particle Image Velocimetry Results Instantaneous Velocity Streamplot Front Step Top Step Time Averaged Velocity Contours Front Step Top Step Turbulence Intensity Contours Front Step Top Step ","permalink":"https://filpill.github.io/projects/2017-06-01-piv-dissertation/","tags":["‚öôÔ∏è Engineering"],"title":"Forward Facing Step - Particle Image Velocimetry"},{"categories":null,"contents":"Retrieve Stored Procedures Takes a snapshot of procedure names and DDL contents from the Google BigQuery.\nWITH project_procedures AS ( select CONCAT(routine_catalog,\u0026#39;.\u0026#39;,routine_schema,\u0026#39;.\u0026#39;,routine_name) AS stored_procedure_path ,\u0026#39;\u0026#34;\u0026#39; || ddl || \u0026#39;\u0026#34;\u0026#39; AS ddl, last_altered from `skyuk-uk-vis-cust-res-d1-lab.stored_procedures.INFORMATION_SCHEMA.ROUTINES` ) SELECT * FROM project_procedures It\u0026rsquo;s worth considering that BigQuery does not save a history of revised Stored Procedures.\nCreating a change history is down to you to create through some kind of Git-based CI/CD or by potentially creating a table with Type 2 SCD\u0026rsquo;s.\nSELECT\njbp.destination_table.project_id || \u0026lsquo;.\u0026rsquo; || jbp.destination_table.dataset_id || \u0026lsquo;.\u0026rsquo; || jbp.destination_table.table_id AS table_name FROM region-eu.INFORMATION_SCHEMA.JOBS_BY_PROJECT jbp ,UNNEST(referenced_tables) rt WHERE job_type = \u0026ldquo;QUERY\u0026rdquo; AND user_email = \u0026ldquo;firstname.lastname@email.com\u0026rdquo; AND DATE(creation_time) \u0026gt;= CURRENT_DATE() ORDER BY creation_time DESC LIMIT 1\nSELECT jbp.job_id ,jbp.creation_time ,jbp.user_email ,jbp.statement_type ,jbp.priority ,jbp.start_time ,jbp.end_time ,jbp.state ,jbp.reservation_id ,jbp.total_bytes_processed ,jbp.total_slot_ms ,jbp.cache_hit ,rt.* ,jbp.destination_table.project_id as destination_project_id ,jbp.destination_table.dataset_id as destination_dataset_id ,jbp.destination_table.table_id as destination_table_id FROM region-eu.INFORMATION_SCHEMA.JOBS_BY_PROJECT jbp ,unnest(referenced_tables) rt WHERE job_type = \u0026ldquo;QUERY\u0026rdquo; AND user_email = \u0026ldquo;filip.livancic@sky.uk\u0026rdquo; AND DATE(creation_time) \u0026gt;= \u0026lsquo;2023-05-17\u0026rsquo;\nORDER BY creation_time DESC\nColumnar Array - Storage Compression BEGIN CREATE OR REPLACE TEMP TABLE array_table AS ( SELECT t.* except(comcast_rolloff_week,weekly_discount_value), ARRAY_AGG( STRUCT(comcast_rolloff_week,weekly_discount_value) ) AS rolloff_data FROM skyuk-uk-vis-cust-res-u1-lab.tableau_broadband_team.bb_offer_rolloff t GROUP BY ALL ); END\nDynamic SQL SET rolloff_weeks = ( SELECT CONCAT(\u0026rsquo;(\u0026quot;\u0026rsquo;, STRING_AGG(DISTINCT comcast_rolloff_week, \u0026lsquo;\u0026quot;, \u0026ldquo;\u0026rsquo;), \u0026lsquo;\u0026rdquo;)\u0026rsquo;) FROM rolloff_calendar ORDER BY 1 ASC );\nEXECUTE IMMEDIATE format(\u0026quot;\u0026quot;\u0026quot; CREATE OR REPLACE TABLE skyuk-uk-vis-cust-res-d1-lab.tableau_broadband_team.bb_offer_rolloff_pivot AS ( SELECT * FROM skyuk-uk-vis-cust-res-d1-lab.tableau_broadband_team.bb_offer_rolloff_tableau PIVOT(SUM(weekly_rolloff_value) FOR comcast_rolloff_week IN %s) )\n\u0026ldquo;\u0026rdquo;\u0026quot;, rolloff_weeks);\nAuto Expire ALTER TABLE project.dataset.table` SET OPTIONS(expiration_timestamp=TIMESTAMP \u0026lsquo;2024-05-01 00:00:00 UTC\u0026rsquo;, description = \u0026lsquo;Temp clone to enable SP testing due to object access issues.\u0026rsquo;) ;\nPermissions select * from skyuk-uk-viewing-pres-prod.region-eu.INFORMATION_SCHEMA.OBJECT_PRIVILEGES where object_name = \u0026lsquo;uk_pres_content_viewing_is\u0026rsquo; and grantee like \u0026lsquo;%consumer%\u0026rsquo; Column Search SELECT * FROM skyuk-uk-viewing-pres-prod.uk_pres_content_viewing_is.INFORMATION_SCHEMA.COLUMNS WHERE column_name like \u0026lsquo;%nk%\u0026rsquo; ORDER BY table_name,column_name DESC;\nExcept Distinct\nIdentifies distinct rows which are not matching between the two tables\nSELECT table_catalog, table_schema, table_name, column_name, data_type, is_nullable, is_partitioning_column, clustering_ordinal_position, ordinal_position, collation_name column_comment FROM \u0026ndash; skyuk-uk-vis-cust-res-p1-lab.key_tables.INFORMATION_SCHEMA.COLUMNS skyuk-uk-customer-pres-dev.uk_pub_customer_assurance_is.INFORMATION_SCHEMA.COLUMNS WHERE lower(table_name) IN (\u0026lsquo;dim_assurance_case_management_guarantee_details\u0026rsquo;)\nEXCEPT DISTINCT\nSELECT table_catalog, table_schema, table_name, column_name, data_type, is_nullable, is_partitioning_column, clustering_ordinal_position, ordinal_position, collation_name column_comment FROM \u0026ndash; skyuk-uk-vis-cust-res-p1-lab.key_tables.INFORMATION_SCHEMA.COLUMNS skyuk-uk-customer-pres-u01.uk_pub_customer_assurance_is.INFORMATION_SCHEMA.COLUMNS WHERE lower(table_name) IN (\u0026lsquo;dim_assurance_case_management_guarantee_details\u0026rsquo; ) ;\nInfo Schema Routines\nThe routine_definition col contains the data for the SP design.\nselect specific_catalog ,specific_schema ,specific_name ,routine_catalog ,routine_schema ,routine_name ,routine_type ,data_type ,routine_body ,routine_definition ,external_language ,is_deterministic ,security_type ,created ,last_altered ,ddl from \u0026ndash;skyuk-uk-vis-cust-res-p1-lab.batch_processing.INFORMATION_SCHEMA.ROUTINES skyuk-uk-vis-cust-res-p1-lab.stored_procedures.INFORMATION_SCHEMA.ROUTINES ; Data Lineage\nSample SQL for \u0026ldquo;dbfiddle\u0026rdquo; designed for postGres database in order trace lineage of a set of id values\nCREATE TABLE test ( id INTEGER, parent_project_id INTEGER );\nINSERT INTO test (id, parent_project_id) VALUES (2, 1), (3, 1), (7, 2), (8, 3), (9, 3), (10, 8), (11, 8);\nCREATE TABLE path_names ( id INTEGER, path_name VARCHAR );\nINSERT INTO path_names (id, path_name) VALUES (1,\u0026lsquo;A\u0026rsquo;), (2,\u0026lsquo;B\u0026rsquo;), (3,\u0026lsquo;C\u0026rsquo;), (4,\u0026lsquo;D\u0026rsquo;), (5,\u0026lsquo;E\u0026rsquo;), (6,\u0026lsquo;F\u0026rsquo;), (7,\u0026lsquo;G\u0026rsquo;), (8,\u0026lsquo;H\u0026rsquo;), (9,\u0026lsquo;I\u0026rsquo;), (10,\u0026lsquo;J\u0026rsquo;), (11,\u0026lsquo;K\u0026rsquo;);\n/* Need to join back to the project name */ WITH RECURSIVE c AS ( \u0026ndash;SELECT 1 AS leaf, \u0026ndash; 1 AS parent, \u0026ndash; 0 AS n \u0026ndash;UNION ALL SELECT id AS leaf, id AS parent, 0 AS n FROM test UNION ALL SELECT leaf, t.parent_project_id AS parent, c.n + 1 FROM test AS t INNER JOIN c ON c.parent = t.id ) SELECT leaf AS id, array_agg(parent ORDER BY n DESC) FILTER (WHERE parent IS NOT NULL) AS ancestry, max(parent) FILTER (WHERE n = 1) AS parent, STRING_AGG(parent::text,\u0026rsquo;\u0026gt;\u0026rsquo; order by n DESC) FILTER (WHERE parent IS NOT NULL) as path FROM c GROUP BY leaf ORDER BY leaf;\n","permalink":"https://filpill.github.io/projects/draft_articles/useful_bq_sql/","tags":null,"title":""},{"categories":null,"contents":"\u0026#x1f636;\u0026zwj;\u0026#x1f32b;\u0026#xfe0f; About Me Aeronautical Engineering Degree Aviation safety management system experience Self-taught maker, programmer and data analyst Hobbies: CAD, 3D Printing, gliding, programming and travelling \u0026#x1f52d; Vision for the Website The purpose of this website is to document my learning journey and aims to:\nContinually build a repository of information to serve as a reference guide for myself and other people in a variety of domains.\nThe true value of knowledge is only unlocked when you are deeply interested in the subject matter and are able to conjure solutions to real world problems.\nYou cannot read random internet articles and then suddenly proclaim \u0026ldquo;I have learned the thing!\u0026rdquo;. It completely dismisses all the stumbling blocks people typically encounter in the learning process. Be real with yourself. Do not delude yourself into believing passive learning is effective.\nIf you are not struggling, you are probably doing it wrong\u0026hellip;\nMy articles are written to provoke thoughts of how you can use technology in the real world. Experiment with your new found knowledge and apply it in different ways. Revisit those concepts several times before a solid understanding emerges.\nI place great value of writing it down your learning process as it helps reinforce and consolidate your new found ideas. You will be a more grounded thinker and it helps connect previously isolated concepts in order to build bigger and greater things.\nThis website is dedicated to documenting and preserving learning\u0026rsquo;s through time.\n\u0026#x1f4c4; Resume Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. ","permalink":"https://filpill.github.io/profile/","tags":null,"title":"üë§ Profile - Filip Livancic"},{"categories":null,"contents":"","permalink":"https://filpill.github.io/archives/","tags":null,"title":"üìÅ Archive"},{"categories":null,"contents":"","permalink":"https://filpill.github.io/search/","tags":null,"title":"Search"}]