<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Understanding dbt Cloud Fundamentals | Filip Livancic</title>
<meta name="keywords" content="üíª Systems">
<meta name="description" content="Covering all the basic aspects of maintaining dbt infrastructure">
<meta name="author" content="">
<link rel="canonical" href="https://filpill.github.io/projects/2024-09-09-learning-dbt/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e2877ab88332b380def17e011d0616342cb0880113e8cd56b93e63d462ca341c.css" integrity="sha256-4od6uIMys4De8X4BHQYWNCywiAET6M1WuT5j1GLKNBw=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://filpill.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://filpill.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://filpill.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://filpill.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://filpill.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://filpill.github.io/projects/2024-09-09-learning-dbt/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://filpill.github.io/projects/2024-09-09-learning-dbt/">
  <meta property="og:site_name" content="Filip Livancic">
  <meta property="og:title" content="Understanding dbt Cloud Fundamentals">
  <meta property="og:description" content="Covering all the basic aspects of maintaining dbt infrastructure">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="projects">
    <meta property="article:published_time" content="2024-09-09T17:44:58+00:00">
    <meta property="article:modified_time" content="2024-09-09T17:44:58+00:00">
    <meta property="article:tag" content="üíª Systems">
    <meta property="og:image" content="https://filpill.github.io/img/dbt/dbt_office.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://filpill.github.io/img/dbt/dbt_office.png">
<meta name="twitter:title" content="Understanding dbt Cloud Fundamentals">
<meta name="twitter:description" content="Covering all the basic aspects of maintaining dbt infrastructure">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "https://filpill.github.io/projects/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Understanding dbt Cloud Fundamentals",
      "item": "https://filpill.github.io/projects/2024-09-09-learning-dbt/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Understanding dbt Cloud Fundamentals",
  "name": "Understanding dbt Cloud Fundamentals",
  "description": "Covering all the basic aspects of maintaining dbt infrastructure",
  "keywords": [
    "üíª Systems"
  ],
  "articleBody": "üì∞ Introduction The dbt tool has garnered a lot of attention in the data ecosystem as it‚Äôs managed to simplify the process of building data pipelines in the transform step.\nThe key selling point of dbt is its ability to abstract/automate away certain DDL processes such as creating tables, so you can focus more strongly on developing business logic which will provide value to your business.\nDeveloping with dbt will encourage you to break complex SQL procedures into smaller composable segments. This has the advantage of being:\nEasier to read Easier to build Easier to maintain Easier to test If you are interested in building data with dbt, I highly recommend taking the dbt fundamentals course in conjunction with reading the documentation which was formative in writing this article.\nYou can check out this repository to see how I structure my personal dbt project: https://github.com/Filpill/dbt_learn\nYou can connect dbt to any database or data warehouse instance, but I will walk through my experience with using the tool and how it integrates with the BigQuery data warehouse.\nüõ†Ô∏è Installing dbt Cloud CLI tool The CLI tool can be a pretty handy way to interface with dbt cloud. You will conveniently be able to run dbt commands through your own customized environment which is neat. And to an extent its more convenient to deal with merge conflicts when you are already embedded in a terminal-like environment.\nIn order to install this utility you would need to ensure you do not have dbt core installed on the machine which would otherwise cause conflicts with running dbt commands.\nInstalling the CLI tool is a straightforward process. You can download the latest installation from: https://github.com/dbt-labs/dbt-cli/releases\nYou will need to make a dbt configuration file which exists in the following path: ~/.dbt/dbt_cloud.yml\nFrom the dbt website you can create your project and account information which can be copied into the aforementioned file. This will allow you to connect to dbt cloud in order to execute commands in your managed environments:\nLastly, the dbt program itself needs to be extracted into the root directory of the git repository storing dbt files. In this setup, dbt commands will only execute from folder its situated in. There is a way of applying this program globally across any directory, but requires some extra system configuration.\nüë• Creating a Service Account When running dbt deployments, its not going to be feasible to run scheduled jobs from a personal user account.\nIdeally you want some kind of service account that is not bound to any particular user or require any extra authentication steps.\nService accounts should have their permissions sufficiently restricted to the tasks they are supposed to perform. At a minimum the account should have BigQuery Editor and BigQuery Job User permissions.\nIn this educational scenario, I‚Äôm using the same service account across the multiple dev/uat/prod projects setups.\nüè≠ Connecting to Data Warehouse Provided you have a service account and GCP project created, you can fill out the connections form on dbt cloud to create the interface.\nSimply attaching the private JSON key from your service account will auto populate all the fields leaving you only needing to fill the project ID field.\nThe default project is stored at the connection level, however, when deploying to different environments, you will be able to specify override conditions to target alternative databases/projects in either the environment or job settings.\nüíæ Connecting to Git Repository Connecting to Git is a straightforward affair, as long as you have repository pre-created, you can simply hook up dbt cloud to it can give it authorisation to modify the repo.\nRepositories exist at a per dbt project level as seen here:\nIf you wish to use segregated git repo‚Äôs, you would need to instantiate more dbt projects.\nThe git integration allows you to easily inspect which files are being added/changed/removed. Overall, it should help you make a more conscious effort in grouping git commits appropriately.\nContinuously developing dbt models alongside production code is core to leveraging the system to its fullest.\nüß± Development File Structure There should be a button on the side bar to initialise a dbt project. On doing so, dbt will instantiate numerous folders which all serve a unique purpose when developing your data pipeline.\nIt may seem a little overwhelming at first, however, this folder structure helps enforce a more uniform development style in organisation.\nAdditionally, it significantly simplifies your SQL into smaller readable chunks which are easier to maintain. Data lineage consequently becomes clearer as you can more easily track data dependencies.\nAll data sources are denoted clearly, with all the connections between staging models and dependencies flowing into production models.\nI‚Äôll list out a brief summary of the components you will most frequently work with:\nConfiguration Files Every dbt project will be initialised with a configuration in the root directory called: *dbt_project.yml.\nThis file houses project level configurations that can globally affect models in the entire repository. For example you can change the materialisation of models in the project to tables as opposed to views (which are the default output). You can also confine the materialisation config to specific model directories.\nProject variables can also be set inside the dbt_project.yml to be used globally across models:\nAdditionally, as you develop out your dbt project, you can add further yaml files to the models directory to set up configurations for source data and for staging data.\nWithin these specific configuration files you will be able to add test cases, documentation, and also data freshness checks for targeted models:\nTesting: Data Freshness Checks: Configuration files are crucial to effectively segmenting, testing and documenting your dbt models.\nModels The models directory is storage for chunks of SQL transformation (SQL dbt models). You can also include sub-folders inside to further organise the work. Examples of sub directories can be:\nstaging: directory for pulling in data from raw/source tables and proceeding to apply intermediate transformations. mart: directory for storing finalised transformations ready for analytics; can be further subdivided into more folders relating to specific business units How to reference various objects:\nData sources: {{ source(‚Äòdataset‚Äô,‚Äôtable‚Äô) }} Staging Tables: {{ ref(‚Äòstg_table‚Äô) }} Variables: {{ var(‚Äòvariable_name‚Äô) }} Example of dbt source/ref/var functions:\nCompiled SQL:\nNotice how we are able to easily replace in the targets very easily. This make deployments to other environments almost trivial to implement.\nRaw BigQuery SQL typically has difficulty in having dynamic project/dataset/tables names (unless you are happy to implement a heavy dose of dynamic SQL) ‚Äì Dbt on the other hand makes this process trivial to deal with.\nMacros If you are writing SQL in a traditional database system, you might see yourself re-writing the same portion across various SQL scripts. User defined functions (UDF) typically alleviate some that repetition to reduce code redundancy.\nDbt macros are much akin to the aforementioned UDF, though you can expect more flexibility and extensibility from macros. But why is that so? ‚Äì SQL written in a dbt model is pre-compiled before executing against the data warehouse. Dbt macros makes use of the Jinja (a pythonic programming language) to dynamically compile your SQL.\nLets imagine an extreme example of writing a gigantic case statement with hundreds of scenarios. Imagine how tedious it is copy pasting all those conditions by hand. Imagine trying to debug that case statement weeks later. Borderline garbage experience‚Ä¶\nWith the use of Jinja you can put all these conditions into a list and iterate through with a loop to compile your SQL. There are a few of advantages:\nLooks compact from a dbt model perspective; typically less lines, and easier to read. Project variables can be shared with other dbt models. Reduces human error when building out dbt models. Here is an example where you can build a list expansion into your SQL compilation.\nMacro definition: Reference to macro in model: Resulting compiled SQL: You will benefit from macros in a range of implementations. It really comes down to a degree of creativity in reducing repeatability.\nTests Dbt tests come in two flavours:\nSingular: Bespoke SQL saved in a SQL file within the tests folder to be executed by the test command. Generic: Pre-built dbt tests which can be applied via the yaml configuration on specified models. unique: Ensuring selected column in model is unique. not_null: Ensuring selected column in model doesn‚Äôt contain any nulls. accepted_values: Ensures the selected column is only composed of values of a defined list. relationships: Verifies referential integrity between 2 tables. Generic tests should take you fairly far in monitoring pipeline data quality. Singular tests should be reserved for special transformation/logic that needs to be rigorously assessed.\nThe dbt testing feature is really powerful, you are able to able to have full test coverage at every stage of your data pipeline and clearly identify quality issues to be fixed.\nüìù Documentation Documentation occurs inside the configuration files to record information about sources, data models, business context, columns, data types and more.\nAdditionally, within the models directory, you are able to create a _docs.md file which can store multi-line text blocks or tables in markdown.\nYou can use the {{ doc(‚Äô‚Äô) }} reference to insert those text blocks. This also ideal for reducing clutter in configuration files.\nAfter writing out your dbt documentation, you can run the following dbt command to compile documents into HTML: dbt docs generate.\nYou can then proceed to review the compiled documents on the dbt cloud interface on the web.\nThis is much better than filing information onto a completely separate system where no doubt somebody will forget to update the docs.\nDbt documentation is a more natural and procedural approach to documenting your data models and overall feels less of chore to do vs other corporate solutions such as confluence. Its also easier to clean up after you decide to deprecate certain dbt projects/solutions. Less rotting documentation‚Ä¶\nRunning documentation directly in dbt keeps it alive and significantly less likely to get orphaned in your development cycles.\nüå• Environments There are a plethora of ways to manage environments in your data warehouse and it‚Äôs completely up to you to decide how to create environmental segmentation.\nDbt will always mandate a dedicated development environment within a project which is where you will be developing your dbt models. Typically, when you run development models, they will land in a user schema/dataset e.g. dbt_filip_livancic. This will isolate your outputs and avoid collisions with other developers building in parallel.\nHowever, when we start to deploy our datasets into production environments, we should avoid inter-mingling development and production schemas/datasets, and possibly trigger different data volumes or model refresh frequencies.\nOne way to create that segmentation is to setup environments in different BigQuery Projects, for example we can define the following projects:\nDevelop - for developing data models UAT - for data quality testing and validation Production - for analytical consumption The term ‚Äúproject‚Äù is used here interchangably with ‚Äúdatabase‚Äù. The terminology is specific to BigQuery.\nIn order to push to separate projects, we must utilise some specific features in dbt to enable it:\nExtended Attributes Inside Extended Attributes you can define configuration overrides which must be defined in YAML.\nWhen creating a dbt production environment for instance, we are able to specify a target name (which refers to the dbt environments) and the project name which relates to the target environment:\nEffectively what we are doing is creating an override to the destination project for production. If we did not specify these overrides, the output would land in the default project you defined in your data warehouse connector.\nEnvironment Variables Additionally, we can to leverage environmental variables to point our objects to the correct deployment environment when we are either developing or productionising:\nThe usage is particularly useful for source objects, where we can use environment variables to point them to dev or prod projects.\nüõ£ Deployments and Jobs When it comes to deploying to different environments, we can leverage these different connections to push to different target environments, which in our case would be different GCP project_id‚Äôs.\nExamples of jobs you can design:\nAn ad-hoc job which manually runs a deployment, can be suitable for landing data in a test environment. A daily-scheduled job which would deploy to a production environment on a daily basis. The dbt scheduler can be a useful option for orchestrating jobs for your data warehouse; it feels fairly natural to monitor and fix pipelines within the same ecosystem.\nüí¨ Closing Commentary This was a fairly rudimentary outline in utilising dbt, however, it was primarily written with the intent of recounting my experience of using dbt.\nThought it more-or-less encompasses all the aspects you would interact with on a daily basis. You can read the further into the official docs for orchestrating a legitimate dbt project.\nBut after my brief experience with the tool, I can see why people convert over to the church of dbt.\n",
  "wordCount" : "2157",
  "inLanguage": "en",
  "image":"https://filpill.github.io/img/dbt/dbt_office.png","datePublished": "2024-09-09T17:44:58Z",
  "dateModified": "2024-09-09T17:44:58Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://filpill.github.io/projects/2024-09-09-learning-dbt/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Filip Livancic",
    "logo": {
      "@type": "ImageObject",
      "url": "https://filpill.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>mermaid.initialize({ startOnLoad: true, securityLevel: 'loose'}});</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://filpill.github.io/" accesskey="h" title="Filip Livancic (Alt + H)">Filip Livancic</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://filpill.github.io/" title="üè† Home">
                    <span>üè† Home</span>
                </a>
            </li>
            <li>
                <a href="https://filpill.github.io/profile/" title="üë§ Profile">
                    <span>üë§ Profile</span>
                </a>
            </li>
            <li>
                <a href="https://filpill.github.io/archives/" title="üìÅ Archive">
                    <span>üìÅ Archive</span>
                </a>
            </li>
            <li>
                <a href="https://filpill.github.io/tags/" title="üè∑Ô∏è Tags">
                    <span>üè∑Ô∏è Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://filpill.github.io/">Home</a>&nbsp;¬ª&nbsp;<a href="https://filpill.github.io/projects/">Projects</a></div>
    <h1 class="post-title">
      Understanding dbt Cloud Fundamentals
    </h1>
    <div class="post-description">
      Covering all the basic aspects of maintaining dbt infrastructure
    </div>
    <div class="post-meta"><span title='2024-09-09 17:44:58 +0000 UTC'>September 9, 2024</span>&nbsp;¬∑&nbsp;11 min&nbsp;¬∑&nbsp;2157 words

</div>
  </header> 
<figure class="entry-cover">
        <img loading="lazy" src="https://filpill.github.io/img/dbt/dbt_office.png" alt="">
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#-introduction" aria-label="üì∞ Introduction">üì∞ Introduction</a><ul>
                        
                <li>
                    <a href="#-installing-dbt-cloud-cli-tool" aria-label="üõ†Ô∏è Installing dbt Cloud CLI tool">üõ†Ô∏è Installing dbt Cloud CLI tool</a></li>
                <li>
                    <a href="#-creating-a-service-account" aria-label="üë• Creating a Service Account">üë• Creating a Service Account</a></li>
                <li>
                    <a href="#-connecting-to-data-warehouse" aria-label="üè≠ Connecting to Data Warehouse">üè≠ Connecting to Data Warehouse</a></li>
                <li>
                    <a href="#-connecting-to-git-repository" aria-label="üíæ Connecting to Git Repository">üíæ Connecting to Git Repository</a></li>
                <li>
                    <a href="#-development-file-structure" aria-label="üß± Development File Structure">üß± Development File Structure</a></li>
                <li>
                    <a href="#-documentation" aria-label="üìù Documentation">üìù Documentation</a></li>
                <li>
                    <a href="#-environments" aria-label="üå• Environments">üå• Environments</a></li>
                <li>
                    <a href="#-deployments-and-jobs" aria-label="üõ£ Deployments and Jobs">üõ£ Deployments and Jobs</a></li>
                <li>
                    <a href="#-closing-commentary" aria-label="üí¨ Closing Commentary">üí¨ Closing Commentary</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="-introduction">üì∞ Introduction<a hidden class="anchor" aria-hidden="true" href="#-introduction">#</a></h1>
<p>The dbt tool has garnered a lot of attention in the data ecosystem as it&rsquo;s managed to simplify the process of building data pipelines in the transform step.</p>
<p>The key selling point of dbt is its ability to abstract/automate away certain DDL processes such as creating tables, so you can focus more strongly on developing business logic which will provide value to your business.</p>
<p>Developing with dbt will encourage you to break complex SQL procedures into smaller composable segments. This has the advantage of being:</p>
<ul>
<li>Easier to read</li>
<li>Easier to build</li>
<li>Easier to maintain</li>
<li>Easier to test</li>
</ul>
<p>If you are interested in building data with dbt, I highly recommend taking the <em><strong><a href="https://learn.getdbt.com/courses/dbt-fundamentals">dbt fundamentals course</a></strong></em> in conjunction with reading the documentation which was formative in writing this article.</p>
<blockquote>
<p>You can check out this repository to see how I structure my personal dbt project: <em><strong><a href="https://github.com/Filpill/dbt_learn">https://github.com/Filpill/dbt_learn</a></strong></em></p></blockquote>
<p>You can connect dbt to any database or data warehouse instance, but I will walk through my experience with using the tool and how it integrates with the <em><strong>BigQuery data warehouse.</strong></em></p>
<h2 id="-installing-dbt-cloud-cli-tool">üõ†Ô∏è Installing dbt Cloud CLI tool<a hidden class="anchor" aria-hidden="true" href="#-installing-dbt-cloud-cli-tool">#</a></h2>
<p>The CLI tool can be a pretty handy way to interface with dbt cloud. You will conveniently be able to run dbt commands through your own customized environment which is neat. And to an extent its more convenient to deal with merge conflicts when you are already embedded in a terminal-like environment.</p>
<p>In order to install this utility you would need to <strong>ensure you do not have dbt core installed on the machine</strong> which would otherwise cause conflicts with running dbt commands.</p>
<p>Installing the CLI tool is a straightforward process. You can download the latest installation from: <em><strong><a href="https://github.com/dbt-labs/dbt-cli/releases">https://github.com/dbt-labs/dbt-cli/releases</a></strong></em></p>
<p>You will need to make a dbt configuration file which exists in the following path: <em><strong>~/.dbt/dbt_cloud.yml</strong></em></p>
<p><img alt="dbt CLI Config Path" loading="lazy" src="/img/dbt/dbt_cli_config.png#center"></p>
<p>From the dbt website you can create your project and account information which can be copied into the aforementioned file. This will allow you to connect to dbt cloud in order to execute commands in your managed environments:</p>
<p><img alt="dbt CLI Config Download Link" loading="lazy" src="/img/dbt/dbt_cli_config_download.png#center"></p>
<p>Lastly, the <em><strong>dbt program</strong></em> itself needs to be extracted into the root directory of the git repository storing dbt files. In this setup, dbt commands will only execute from folder its situated in. There is a way of applying this program globally across any directory, but requires some extra system configuration.</p>
<p><img alt="dbt CLI" loading="lazy" src="/img/dbt/dbt_cli.png#center"></p>
<h2 id="-creating-a-service-account">üë• Creating a Service Account<a hidden class="anchor" aria-hidden="true" href="#-creating-a-service-account">#</a></h2>
<p>When running dbt deployments, its not going to be feasible to run scheduled jobs from a personal user account.</p>
<p>Ideally you want some kind of <strong>service account</strong> that is not bound to any particular user or require any extra authentication steps.</p>
<p>Service accounts should have their permissions sufficiently restricted to the tasks they are supposed to perform. At a minimum the account should have <strong>BigQuery Editor</strong> and <strong>BigQuery Job User</strong> permissions.</p>
<p><img alt="GCP Service Account Permissons" loading="lazy" src="/img/dbt/gcp_service_account_perms.png#center"></p>
<blockquote>
<p><em><strong>In this educational scenario, I&rsquo;m using the same service account across the multiple dev/uat/prod projects setups.</strong></em></p></blockquote>
<h2 id="-connecting-to-data-warehouse">üè≠ Connecting to Data Warehouse<a hidden class="anchor" aria-hidden="true" href="#-connecting-to-data-warehouse">#</a></h2>
<p>Provided you have a service account and GCP project created, you can fill out the connections form on dbt cloud to create the interface.</p>
<p>Simply attaching the private JSON key from your service account will auto populate all the fields leaving you only needing to fill the project ID field.</p>
<p><img alt="GCP Connection Detail" loading="lazy" src="/img/dbt/gcp_connection_detail.png#center"></p>
<p>The default project is stored at the connection level, however, when deploying to different environments, you will be able to specify override conditions to target alternative databases/projects in either the environment or job settings.</p>
<h2 id="-connecting-to-git-repository">üíæ Connecting to Git Repository<a hidden class="anchor" aria-hidden="true" href="#-connecting-to-git-repository">#</a></h2>
<p>Connecting to Git is a straightforward affair, as long as you have repository pre-created, you can simply hook up dbt cloud to it can give it authorisation to modify the repo.</p>
<p>Repositories exist at a per dbt project level as seen here:</p>
<p><img alt="dbt project details" loading="lazy" src="/img/dbt/dbt_project_details.png#center"></p>
<p>If you wish to use segregated git repo&rsquo;s, you would need to instantiate more dbt projects.</p>
<p>The git integration allows you to easily inspect which files are being added/changed/removed. Overall, it should help you make a more conscious effort in grouping git commits appropriately.</p>
<p>Continuously developing dbt models alongside production code is core to leveraging the system to its fullest.</p>
<h2 id="-development-file-structure">üß± Development File Structure<a hidden class="anchor" aria-hidden="true" href="#-development-file-structure">#</a></h2>
<p>There should be a button on the side bar to initialise a dbt project. On doing so, dbt will instantiate numerous folders which all serve a unique purpose when developing your data pipeline.</p>
<p><img alt="dbt file structure" loading="lazy" src="/img/dbt/dbt_file_structure.png#center"></p>
<p>It may seem a little overwhelming at first, however, this folder structure helps enforce a more uniform development style in organisation.</p>
<p>Additionally, it significantly simplifies your SQL into smaller readable chunks which are easier to maintain. Data lineage consequently becomes clearer as you can more easily track data dependencies.</p>
<p>All data sources are denoted clearly, with all the connections between staging models and dependencies flowing into production models.</p>
<p><img alt="dbt data lineage" loading="lazy" src="/img/dbt/dbt_data_lineage.png#center"></p>
<p>I&rsquo;ll list out a brief summary of the components you will most frequently work with:</p>
<h4 id="hahahugoshortcode24s0hbhb"><span style="
      background-color:#702963;
      color:#FFF;
      padding:1px;
      border-radius:5px;
  ">
    &nbsp;Configuration Files 
</span>
&nbsp;
</h4>
<p>Every dbt project will be initialised with a configuration in the root directory called: *<strong>dbt_project.yml</strong>.</p>
<p>This file houses project level configurations that can globally affect models in the entire repository. For example you can change the materialisation of models in the project to <em>tables</em> as opposed to views (which are the default output). You can also confine the materialisation config to specific model directories.</p>
<p><img alt="dbt project materilisation" loading="lazy" src="/img/dbt/dbt_project_material.png#center"></p>
<p>Project variables can also be set inside the <em><strong>dbt_project.yml</strong></em> to be used globally across models:</p>
<p><img alt="dbt project variables" loading="lazy" src="/img/dbt/dbt_project_vars.png#center"></p>
<p>Additionally, as you develop out your dbt project, you can add further yaml files to the <strong>models</strong> directory to set up configurations for source data and for staging data.</p>
<p>Within these specific configuration files you will be able to add test cases, documentation, and also data freshness checks for targeted models:</p>
<p>Testing:
<img alt="dbt generic testing" loading="lazy" src="/img/dbt/dbt_yaml_test.png#center"></p>
<p>Data Freshness Checks:
<img alt="dbt data freshness check" loading="lazy" src="/img/dbt/dbt_data_freshness.png#center"></p>
<p>Configuration files are crucial to effectively segmenting, testing and documenting your dbt models.</p>
<h4 id="hahahugoshortcode24s1hbhb"><span style="
      background-color:#8ec1f5;
      color:#0A0A0A;
      padding:1px;
      border-radius:5px;
  ">
    &nbsp;Models 
</span>
&nbsp;
</h4>
<p>The models directory is storage for chunks of SQL transformation (SQL dbt models). You can also include sub-folders inside to further organise the work. Examples of sub directories can be:</p>
<ul>
<li><strong>staging:</strong> directory for pulling in data from raw/source tables and proceeding to apply intermediate transformations.</li>
<li><strong>mart:</strong> directory for storing finalised transformations ready for analytics; can be further subdivided into more folders relating to specific business units</li>
</ul>
<p><strong>How to reference various objects:</strong></p>
<ul>
<li>Data sources: <em><strong>{{ source(&lsquo;dataset&rsquo;,&rsquo;table&rsquo;) }}</strong></em></li>
<li>Staging Tables: <em><strong>{{ ref(&lsquo;stg_table&rsquo;) }}</strong></em></li>
<li>Variables: <em><strong>{{ var(&lsquo;variable_name&rsquo;) }}</strong></em></li>
</ul>
<p>Example of dbt source/ref/var functions:</p>
<p><img alt="dbt data ref funcs" loading="lazy" src="/img/dbt/dbt_reference_func.png#center"></p>
<p>Compiled SQL:</p>
<p><img alt="dbt data ref funcs" loading="lazy" src="/img/dbt/dbt_reference_compiled.png#center"></p>
<p>Notice how we are able to easily replace in the targets very easily. This make deployments to other environments almost trivial to implement.</p>
<blockquote>
<p><em><strong>Raw BigQuery SQL typically has difficulty in having dynamic project/dataset/tables names (unless you are happy to implement a heavy dose of dynamic SQL) &ndash; Dbt on the other hand makes this process trivial to deal with.</strong></em></p></blockquote>
<h4 id="hahahugoshortcode24s2hbhb"><span style="
      background-color:#f5eb5b;
      color:#000;
      padding:1px;
      border-radius:5px;
  ">
    &nbsp;Macros 
</span>
&nbsp;
</h4>
<p>If you are writing SQL in a traditional database system, you might see yourself re-writing the same portion across various SQL scripts. <strong>User defined functions</strong> (UDF) typically alleviate some that repetition to reduce code redundancy.</p>
<p>Dbt macros are much akin to the aforementioned UDF, though you can expect more flexibility and extensibility from macros. <em>But why is that so?</em> &ndash; <em><strong>SQL written in a dbt model is pre-compiled before executing against the data warehouse.</strong></em> Dbt macros makes use of the Jinja (a pythonic programming language) to dynamically compile your SQL.</p>
<p>Lets imagine an extreme example of writing a gigantic case statement with hundreds of scenarios. Imagine how tedious it is copy pasting all those conditions by hand. Imagine trying to debug that case statement weeks later. <em>Borderline garbage experience&hellip;</em></p>
<p>With the use of Jinja you can put all these conditions into a list and iterate through with a loop to compile your SQL. There are a few of advantages:</p>
<ul>
<li><strong>Looks compact</strong> from a dbt model perspective; typically less lines, and easier to read.</li>
<li><strong>Project variables</strong> can be shared with other dbt models.</li>
<li><strong>Reduces human error</strong> when building out dbt models.</li>
</ul>
<p>Here is an example where you can build a list expansion into your SQL compilation.</p>
<p><strong>Macro definition:</strong>
<img alt="dbt macro example" loading="lazy" src="/img/dbt/dbt_expand_list_macro.png#center"></p>
<p><strong>Reference to macro in model:</strong>
<img alt="dbt macro reference" loading="lazy" src="/img/dbt/dbt_macro_reference.png#center"></p>
<p><strong>Resulting compiled SQL:</strong>
<img alt="dbt macro compiled SQL" loading="lazy" src="/img/dbt/dbt_macro_compiled.png#center"></p>
<blockquote>
<p><em><strong>You will benefit from macros in a range of implementations. It really comes down to a degree of creativity in reducing repeatability.</strong></em></p></blockquote>
<h4 id="hahahugoshortcode24s3hbhb"><span style="
      background-color:#7feb4d;
      color:#000;
      padding:1px;
      border-radius:5px;
  ">
    &nbsp;Tests 
</span>
&nbsp;
</h4>
<p><strong>Dbt tests come in two flavours:</strong></p>
<ul>
<li><strong>Singular:</strong> Bespoke SQL saved in a SQL file within the tests folder to be executed by the test command.</li>
<li><strong>Generic:</strong> Pre-built dbt tests which can be applied via the yaml configuration on specified models.
<ul>
<li><strong>unique:</strong> Ensuring selected column in model is unique.</li>
<li><strong>not_null:</strong> Ensuring selected column in model doesn&rsquo;t contain any nulls.</li>
<li><strong>accepted_values:</strong> Ensures the selected column is only composed of values of a defined list.</li>
<li><strong>relationships:</strong> Verifies referential integrity between 2 tables.</li>
</ul>
</li>
</ul>
<p><img alt="dbt test" loading="lazy" src="/img/dbt/dbt_test.png#center"></p>
<p>Generic tests should take you fairly far in monitoring pipeline data quality. <strong>Singular tests</strong> should be reserved for <strong>special transformation/logic that needs to be rigorously assessed.</strong></p>
<blockquote>
<p>The dbt testing feature is really powerful, you are able to able to have full test coverage at every stage of your data pipeline and clearly identify quality issues to be fixed.</p></blockquote>
<h2 id="-documentation">üìù Documentation<a hidden class="anchor" aria-hidden="true" href="#-documentation">#</a></h2>
<p>Documentation occurs inside the configuration files to record information about sources, data models, business context, columns, data types and more.</p>
<p>Additionally, within the <strong>models</strong> directory, you are able to create a <em><strong>_docs.md</strong></em> file which can store multi-line text blocks or tables in markdown.</p>
<p><img alt="dbt document block" loading="lazy" src="/img/dbt/dbt_doc_block.png#center"></p>
<p>You can use the <strong>{{ doc(&rsquo;&rsquo;) }}</strong> reference to insert those text blocks. This also ideal for reducing clutter in configuration files.</p>
<p><img alt="dbt document block insertion" loading="lazy" src="/img/dbt/dbt_doc_block_insert.png#center"></p>
<p>After writing out your dbt documentation, you can run the following dbt command to compile documents into HTML: <em><strong>dbt docs generate</strong></em>.</p>
<p><img alt="dbt document generation" loading="lazy" src="/img/dbt/dbt_docs_generate.png#center"></p>
<p>You can then proceed to review the compiled documents on the dbt cloud interface on the web.</p>
<p>This is much better than filing information onto a completely separate system where no doubt somebody will forget to update the docs.</p>
<p>Dbt documentation is a more natural and procedural approach to documenting your data models and overall feels less of chore to do vs other corporate solutions such as <strong>confluence</strong>. Its also easier to clean up after you decide to deprecate certain dbt projects/solutions. <em>Less rotting documentation&hellip;</em></p>
<blockquote>
<p><em><strong>Running documentation directly in dbt keeps it alive and significantly less likely to get orphaned in your development cycles.</strong></em></p></blockquote>
<h2 id="-environments">üå• Environments<a hidden class="anchor" aria-hidden="true" href="#-environments">#</a></h2>
<p>There are a plethora of ways to manage environments in your data warehouse and it&rsquo;s completely up to you to decide how to create environmental segmentation.</p>
<p>Dbt will <strong>always mandate a dedicated development environment</strong> within a project which is where you will be developing your dbt models. Typically, when you run development models, they will land in a user schema/dataset e.g. <strong>dbt_filip_livancic</strong>. This will isolate your outputs and avoid collisions with other developers building in parallel.</p>
<p><img alt="dbt develop creds" loading="lazy" src="/img/dbt/dbt_develop_credentials.png#center"></p>
<p>However, when we start to deploy our datasets into production environments, we should avoid inter-mingling development and production schemas/datasets, and possibly trigger different data volumes or model refresh frequencies.</p>
<p>One way to create that segmentation is to setup environments in different BigQuery Projects, for example we can define the following projects:</p>
<ul>
<li><strong>Develop</strong> - for developing data models</li>
<li><strong>UAT</strong> - for data quality testing and validation</li>
<li><strong>Production</strong> - for analytical consumption</li>
</ul>
<blockquote>
<p><em><strong>The term &ldquo;project&rdquo; is used here interchangably with &ldquo;database&rdquo;. The terminology is specific to BigQuery.</strong></em></p></blockquote>
<p>In order to push to separate projects, we must utilise some specific features in dbt to enable it:</p>
<h4 id="hahahugoshortcode24s4hbhb"><span style="
      background-color:#006666;
      color:#FFF;
      padding:1px;
      border-radius:5px;
  ">
    &nbsp;Extended Attributes 
</span>
&nbsp;
</h4>
<p>Inside <strong>Extended Attributes</strong> you can define configuration overrides which must be defined in YAML.</p>
<p>When creating a dbt production environment for instance, we are able to specify a target name (which refers to the dbt environments) and the project name which relates to the target environment:</p>
<p><img alt="dbt extended attributes" loading="lazy" src="/img/dbt/dbt_extended_attr_target.png#center"></p>
<p>Effectively what we are doing is creating an override to the destination project for production. If we did not specify these overrides, the output would land in the default project you defined in your data warehouse connector.</p>
<h4 id="hahahugoshortcode24s5hbhb"><span style="
      background-color:#a98600;
      color:#000;
      padding:1px;
      border-radius:5px;
  ">
    &nbsp;Environment Variables 
</span>
&nbsp;
</h4>
<p>Additionally, we can to leverage environmental variables to point our objects to the correct deployment environment when we are either developing or productionising:</p>
<p><img alt="dbt environment variable" loading="lazy" src="/img/dbt/dbt_saved_env_var.png#center"></p>
<p>The usage is particularly useful for source objects, where we can use environment variables to point them to dev or prod projects.</p>
<p><img alt="dbt source environment" loading="lazy" src="/img/dbt/dbt_src_env_var.png#center"></p>
<h2 id="-deployments-and-jobs">üõ£ Deployments and Jobs<a hidden class="anchor" aria-hidden="true" href="#-deployments-and-jobs">#</a></h2>
<p>When it comes to deploying to different environments, we can leverage these different connections to push to different target environments, which in our case would be different GCP project_id&rsquo;s.</p>
<p>Examples of jobs you can design:</p>
<ul>
<li>An <strong>ad-hoc</strong> job which manually runs a deployment, can be suitable for landing data in a test environment.</li>
<li>A <strong>daily-scheduled</strong> job which would deploy to a production environment on a daily basis.</li>
</ul>
<p><img alt="dbt jobs" loading="lazy" src="/img/dbt/dbt_jobs.png#center"></p>
<p>The dbt scheduler can be a useful option for orchestrating jobs for your data warehouse; it feels fairly natural to monitor and fix pipelines within the same ecosystem.</p>
<h2 id="-closing-commentary">üí¨ Closing Commentary<a hidden class="anchor" aria-hidden="true" href="#-closing-commentary">#</a></h2>
<p>This was a fairly rudimentary outline in utilising dbt, however, it was primarily written with the intent of recounting my experience of using dbt.</p>
<p>Thought it more-or-less encompasses all the aspects you would interact with on a daily basis. You can read the further into the official docs for orchestrating a legitimate dbt project.</p>
<p>But after my brief experience with the tool, I can see why people convert over to the <em>church of dbt</em>.</p>



  </div>


  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://filpill.github.io/tags/-systems/">üíª Systems</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://filpill.github.io/projects/2025-03-15-arch-nvidia-drivers/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>Arch Linux - Nvidia Driver Installation</span>
  </a>
  <a class="next" href="https://filpill.github.io/projects/2024-05-21-flight-tracking/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>Flight Tracking Animations</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://filpill.github.io/">Filip Livancic</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
